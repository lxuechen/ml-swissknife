{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WC4dLTtnxD0I"
   },
   "source": [
    "# Template for Homework 2, EE 277"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "YTJniRDFw6S-"
   },
   "outputs": [],
   "source": [
    "#@title imports \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_67yA44BxRFm"
   },
   "source": [
    "## Problem 1: XOR Logistic Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwmWS5i3xV5C"
   },
   "source": [
    "### Part 0: XOR Logistic Bandit Environment\n",
    "\n",
    "In this part, we implement the XOR function and the XOR logistic bandit environment.\n",
    "\n",
    "**You do not need to modify the code in this part.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "98EX_uAexVrM"
   },
   "outputs": [],
   "source": [
    "\"\"\" XOR function\"\"\"\n",
    "\n",
    "def XOR(x, y):\n",
    "  if (x == 0 and y == 1) or (x==1 and y==0):\n",
    "    return 1\n",
    "  else:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "l2-i26HAxHwc"
   },
   "outputs": [],
   "source": [
    "\"\"\" The environment for XOR logistic bandit\"\"\"\n",
    "\n",
    "class XORLogisticBandit:\n",
    "\n",
    "  \"\"\"\n",
    "  The constructor samples an XOR logistic bandit environment\n",
    "  where\n",
    "  1) num_actions is the number of actions (K in the problem statement)\n",
    "  2) dim is the feature dimension (N in the problem statement)\n",
    "  3) the prior distribution for theta is N(0, sigma^2 I)\n",
    "  \"\"\"\n",
    "  def __init__(self, num_actions, dim, sigma_p):\n",
    "\n",
    "    # number of actions\n",
    "    self.num_actions = num_actions\n",
    "\n",
    "    # feature dimension\n",
    "    self.dim = dim\n",
    "\n",
    "    # sample theta and generate environment\n",
    "    self.sigma_p = sigma_p\n",
    "    self.sigma_p_squared = sigma_p**2\n",
    "\n",
    "    # theta (to be learned)\n",
    "    self.theta = sigma_p * np.random.randn(dim)\n",
    "\n",
    "    # feature matrix, feature.shape=(num_actions, dim)\n",
    "    feature = np.random.randn(num_actions, dim)\n",
    "    self.feature = feature/np.linalg.norm(feature, axis=-1, keepdims=True)\n",
    "\n",
    "    exp_logits = np.exp(np.tensordot(self.feature, \\\n",
    "                                     self.theta, axes=([-1], [0])))\n",
    "\n",
    "    # expected reward for all context-action pairs\n",
    "    # self.rewards.shape = (num_actions)\n",
    "    self.p = exp_logits/(1 + exp_logits)\n",
    "    self.rewards = 2 * np.multiply(self.p, 1-self.p)\n",
    "\n",
    "    # values (optimal reward at each context)\n",
    "    # self.values.shape = (1)\n",
    "    self.values = np.max(self.rewards, axis=0, keepdims=True)\n",
    "\n",
    "    # expected instantanesou regret for all context-action pairs\n",
    "    # self.regrets.shape = (num_actions)\n",
    "    self.regrets = self.values - self.rewards\n",
    "\n",
    "  # return the feature matrix of the environment\n",
    "  def get_feature(self):\n",
    "    return self.feature\n",
    "\n",
    "  # agents choose actions by calling this method\n",
    "  def act(self, action):\n",
    "\n",
    "    # assert that action is in the right range\n",
    "    assert action >= 0 and action < self.num_actions\n",
    "\n",
    "    # the instantaneous regret \n",
    "    inst_regret = self.regrets[action]\n",
    "\n",
    "    # probability of observing 1\n",
    "    p_1 = self.p[action]\n",
    "    obs = (np.random.binomial(1, p_1), np.random.binomial(1, p_1))\n",
    "\n",
    "    # reward, which is XOR of the two observations\n",
    "    reward = XOR(obs[0], obs[1])\n",
    "\n",
    "    # returns observation, realized reward, and inst_regret\n",
    "    return obs, reward, inst_regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhjpc3kfxdv-"
   },
   "source": [
    "### Part a: Epsilon-Greedy Agent\n",
    "\n",
    "**You need to implement the select_action method for epsilon-greedy agent.**\n",
    "\n",
    "**Technical Issue:** In case you are interested, we use LogisticRegression module from sklearn to solve the logistic regression. LogisticRegression module requires data with both observation 0 and observation 1. Thus, the agent will start solving logistic regressions once it has seen at least one obervation 0 and one observation 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "IpjOT0mrxa_8"
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedyAgent:\n",
    "\n",
    "  # environment is an XORLogisticBandit\n",
    "  def __init__(self, env):\n",
    "    self.env = env\n",
    "    self.num_actions = self.env.num_actions\n",
    "    self.dim = self.env.dim\n",
    "    self.sigma_p_squared = self.env.sigma_p_squared\n",
    "\n",
    "    # the logistic regression solver\n",
    "    # notice that the choice of L2 penelty and C adds a quadratic regularization\n",
    "    # term corresponding to the prior\n",
    "    # we do not need an intercept in the logistic regression model\n",
    "    # warm_start will use the previous solution as the starting point, which \n",
    "    # will speed up learning\n",
    "    self.solver = LogisticRegression(penalty='l2', \n",
    "                                     C=(2*self.sigma_p_squared),\n",
    "                                     fit_intercept=False,\n",
    "                                     warm_start=True)\n",
    "    \n",
    "\n",
    "  ## theta_est: the current point estimate of theta\n",
    "  ## theta_est.shape = (dim)\n",
    "  ## epsilon is the epsilon in epsilon-greedy algorithms\n",
    "  ## phi is a matrix encoding the action features\n",
    "  ## phi.shape = (num_actions, dim)\n",
    "  def select_action(self, theta_est, epsilon, phi):\n",
    "    ############### Start of Code Modification ################################\n",
    "    ### Please implement the action selection strategy for epsilon-greedy agent\n",
    "    ###########################################################################\n",
    "    if np.random.rand() < epsilon:  # explore\n",
    "        return np.random.randint(low=0, high=self.num_actions)\n",
    "    else:  # exploit\n",
    "        action_potentials = np.abs(phi @ theta_est)\n",
    "        return np.argmin(action_potentials)\n",
    "    ################### End of Code Modification ##############################\n",
    "  \n",
    "  def run(self, horizon=50, epsilon=0.05, verbose=False):\n",
    "    \"\"\"run simulation.\"\"\"\n",
    "\n",
    "    # placeholder for the results\n",
    "    regret_vec = np.zeros(horizon)\n",
    "    cum_regret = 0\n",
    "\n",
    "    # placeholder for used features and observed rewards\n",
    "    X = np.zeros((2*horizon, self.dim))\n",
    "    y = np.zeros(2*horizon)\n",
    "\n",
    "    # initial estimated theta\n",
    "    theta_est = np.zeros(self.dim)\n",
    "\n",
    "    # initialization_phase will be false once we observe both \n",
    "    # an observation 0 and an observation 1\n",
    "    initialization_phase = True\n",
    "\n",
    "    # get features\n",
    "    phi = self.env.get_feature()\n",
    "\n",
    "    for t in range(horizon):\n",
    "\n",
    "      # select action based on epsilon-greedy\n",
    "      action = self.select_action(theta_est, epsilon, phi)\n",
    "\n",
    "      # apply the chosen action\n",
    "      obs, reward, inst_regret  = self.env.act(action)\n",
    "\n",
    "      # update regret\n",
    "      regret_vec[t] = inst_regret\n",
    "      cum_regret += inst_regret\n",
    "\n",
    "      # update the placeholder\n",
    "      X[2*t,:] = phi[action, :]; X[2*t+1,:] = phi[action, :];\n",
    "      y[2*t] = obs[0]; y[2*t+1] = obs[1]\n",
    "\n",
    "      if verbose and (t+1)%100==0:\n",
    "        print(\"Epsilon-Greedy Agent: the cum regret by t={} is {}\".format(t, cum_regret))\n",
    "\n",
    "      # check if we have observations with both reward 0 and 1\n",
    "      if initialization_phase and len(np.unique(y[:(t+1)])) == 2:\n",
    "        initialization_phase = False\n",
    "        if verbose:\n",
    "          print(\"Initialization Phase ends in time step {}\".format(t))\n",
    "        \n",
    "      if not initialization_phase:\n",
    "        # run logistic regression and obtain the learned theta\n",
    "        self.solver.fit(X[:(2*t+2),:], y[:(2*t+2)])\n",
    "        theta_est = self.solver.coef_.reshape(self.dim)\n",
    "\n",
    "    return regret_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4SMjDvZxkt0"
   },
   "source": [
    "### Part b: Approximate Thompson Sampling Agent Based on Laplace Approximation\n",
    "\n",
    "**You need to implement the select_action method for Gaussian approximate Thompson sampling agent.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "kqR5PFJjxf3z"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function computes the inverse Hessian matrix of the loss function\n",
    "theta.shape = (dim)\n",
    "X.shape = (num_samples, dim)\n",
    "'''\n",
    "def getInvHessian(theta, X, sigma_p_squared):\n",
    "\n",
    "  H = np.eye(len(theta))/sigma_p_squared\n",
    "\n",
    "  if X.shape[0] > 0:\n",
    "    logits = np.dot(X, theta)\n",
    "    tmp = np.exp(logits); y_pred = np.divide(tmp, 1+tmp)\n",
    "    D = np.diag(np.multiply(y_pred, 1-y_pred))\n",
    "\n",
    "    H += np.dot(np.dot(X.T, D), X) \n",
    "\n",
    "  return np.linalg.inv(H)\n",
    "\n",
    "\n",
    "class ApproximateTSAgent:\n",
    "\n",
    "  # environment is an XORLogisticBandit\n",
    "  def __init__(self, env):\n",
    "    self.env = env\n",
    "    self.num_actions = self.env.num_actions\n",
    "    self.dim = self.env.dim\n",
    "    self.sigma_p_squared = self.env.sigma_p_squared\n",
    "\n",
    "    # the logistic regression solver\n",
    "    # notice that the choice of L2 penelty and C adds a quadratic regularization\n",
    "    # term corresponding to the prior\n",
    "    # we do not need an intercept in the logistic regression model\n",
    "    # warm_start will use the previous solution as the starting point, which \n",
    "    # will speed up learning\n",
    "    self.solver = LogisticRegression(penalty='l2', \n",
    "                                     C=(2*self.sigma_p_squared),\n",
    "                                     fit_intercept=False,\n",
    "                                     warm_start=True)\n",
    "    \n",
    "\n",
    "  ## mu is the mean of Gaussian approximation\n",
    "  ## mu.shape = (dim)\n",
    "  ## Sigma is the covariance matrix of Gaussian approximation\n",
    "  ## Sigma.shape = (dim, dim)\n",
    "  ## phi is a matrix encoding the action features\n",
    "  ## phi.shape = (num_actions, dim)\n",
    "  def select_action(self, mu, Sigma, phi):\n",
    "    ############### Start of Code Modification ################################\n",
    "    ### Please implement the action selection strategy for approximate TS\n",
    "    ### agent\n",
    "    ###########################################################################\n",
    "    theta = np.random.multivariate_normal(mu, Sigma)\n",
    "    action_potentials = np.abs(phi @ theta)\n",
    "    return np.argmin(action_potentials)\n",
    "    ################### End of Code Modification ##############################\n",
    "  \n",
    "  def run(self, horizon=50, verbose=False):\n",
    "    \"\"\"run simulation.\"\"\"\n",
    "\n",
    "    # placeholder for the results\n",
    "    regret_vec = np.zeros(horizon)\n",
    "    cum_regret = 0\n",
    "\n",
    "    # placeholder for used features and observed rewards\n",
    "    X = np.zeros((2*horizon, self.dim))\n",
    "    y = np.zeros(2*horizon)\n",
    "\n",
    "    # initial mu and Sigma\n",
    "    mu = np.zeros(self.dim)\n",
    "    Sigma = self.sigma_p_squared*np.eye(self.dim)\n",
    "\n",
    "    # initialization_phase will be false once we observe both a reward 0 and\n",
    "    # a reward 1\n",
    "    initialization_phase = True\n",
    "\n",
    "    # get features\n",
    "    phi = self.env.get_feature()\n",
    "\n",
    "    for t in range(horizon):\n",
    "\n",
    "      # select action based on epsilon-greedy\n",
    "      action = self.select_action(mu, Sigma, phi)\n",
    "\n",
    "      # apply the chosen action\n",
    "      obs, reward, inst_regret  = self.env.act(action)\n",
    "\n",
    "      # update regret\n",
    "      regret_vec[t] = inst_regret\n",
    "      cum_regret += inst_regret\n",
    "\n",
    "      # update the placeholder\n",
    "      X[2*t,:] = phi[action, :]; X[2*t+1,:] = phi[action, :];\n",
    "      y[2*t] = obs[0]; y[2*t+1] = obs[1]\n",
    "\n",
    "      if verbose and (t+1)%100==0:\n",
    "        print(\"Laplace Agent: the cum regret by t={} is {}\".format(t, cum_regret))\n",
    "\n",
    "      # check if we have observations with both reward 0 and 1\n",
    "      if initialization_phase and len(np.unique(y[:(t+1)])) == 2:\n",
    "        initialization_phase = False\n",
    "        if verbose:\n",
    "          print(\"Initialization Phase ends in time step {}\".format(t))\n",
    "        \n",
    "      if not initialization_phase:\n",
    "        # run logistic regression and obtain the learned theta\n",
    "        self.solver.fit(X[:(2*t+2),:], y[:(2*t+2)])\n",
    "        mu = self.solver.coef_.reshape(self.dim)\n",
    "\n",
    "        # compute the covariance matrix\n",
    "        Sigma = getInvHessian(mu, X[:(2*t+2),:], self.sigma_p_squared)\n",
    "\n",
    "    return regret_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJzdI6VbxrrR"
   },
   "source": [
    "### Part d: Show Results\n",
    "\n",
    "**You do not need to modify the code below. You just need to run it to generate the plot for the cumulative regrets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "xyLh5l8oxnUv"
   },
   "outputs": [],
   "source": [
    "# function to run the experiments\n",
    "def compareAgents(seed_vec, num_actions, dim, horizon, sigma_p=1):\n",
    "\n",
    "  # placeholders for the results\n",
    "  regret_matrix_eps_greedy = np.zeros((len(seed_vec), horizon))\n",
    "  regret_matrix_laplace = np.zeros((len(seed_vec), horizon))\n",
    "\n",
    "  # loop over the random seeds\n",
    "  for i in range(len(seed_vec)):\n",
    "\n",
    "    np.random.seed(seed_vec[i])\n",
    "    print('For seed={} with horizon {}, the total regrets are:'.format(seed_vec[i], horizon))\n",
    "\n",
    "    # randomly sample one environment\n",
    "    env = XORLogisticBandit(num_actions = num_actions,\n",
    "                        dim = dim, sigma_p = sigma_p)\n",
    "    \n",
    "    # epsilon-greedy agent\n",
    "    agent_epsilon = EpsilonGreedyAgent(env)\n",
    "    regret_matrix_eps_greedy[i,:] = agent_epsilon.run(horizon=horizon, epsilon=0.05)\n",
    "    print('Epsilon-Greedy: {}'.format(np.sum(regret_matrix_eps_greedy[i, :])))\n",
    "\n",
    "    # approximate Thompson sampling agent\n",
    "    agent_laplace = ApproximateTSAgent(env)\n",
    "    regret_matrix_laplace[i,:] = agent_laplace.run(horizon=horizon)\n",
    "    print('Approximate TS: {}'.format(np.sum(regret_matrix_laplace[i, :])))\n",
    "\n",
    "  # compute the mean cumulative regrets\n",
    "  cum_regret = {}\n",
    "  cum_regret['epsilon-greedy'] = np.cumsum(np.mean(regret_matrix_eps_greedy, axis=0))\n",
    "  cum_regret['ats'] = np.cumsum(np.mean(regret_matrix_laplace, axis=0))\n",
    "\n",
    "  return cum_regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "LN5XH8M-xuxX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For seed=0 with horizon 1000, the total regrets are:\n",
      "Epsilon-Greedy: 8.839077695401413\n",
      "Approximate TS: 5.0925457981886515\n",
      "For seed=1 with horizon 1000, the total regrets are:\n",
      "Epsilon-Greedy: 10.670136734257305\n",
      "Approximate TS: 6.197358753805368\n",
      "For seed=2 with horizon 1000, the total regrets are:\n",
      "Epsilon-Greedy: 10.69060677788329\n",
      "Approximate TS: 5.093673438087084\n",
      "For seed=3 with horizon 1000, the total regrets are:\n",
      "Epsilon-Greedy: 9.034895901633181\n",
      "Approximate TS: 5.508309893816031\n",
      "For seed=4 with horizon 1000, the total regrets are:\n",
      "Epsilon-Greedy: 3.3681171663960927\n",
      "Approximate TS: 5.262853726787965\n",
      "For seed=5 with horizon 1000, the total regrets are:\n",
      "Epsilon-Greedy: 12.429480310983504\n",
      "Approximate TS: 8.83577648403709\n",
      "For seed=6 with horizon 1000, the total regrets are:\n",
      "Epsilon-Greedy: 5.4380853167136\n",
      "Approximate TS: 3.984941716241779\n",
      "For seed=7 with horizon 1000, the total regrets are:\n",
      "Epsilon-Greedy: 8.006628720154977\n",
      "Approximate TS: 5.819723148053136\n",
      "For seed=8 with horizon 1000, the total regrets are:\n",
      "Epsilon-Greedy: 13.70853460819503\n",
      "Approximate TS: 4.804796075765308\n",
      "For seed=9 with horizon 1000, the total regrets are:\n",
      "Epsilon-Greedy: 3.897156132991336\n",
      "Approximate TS: 2.7409143444077992\n"
     ]
    }
   ],
   "source": [
    "## Experiment set up\n",
    "\n",
    "# 10 simulations, with random seed = 0,1, ... 9\n",
    "seed_vec = range(10)\n",
    "\n",
    "# 100 actions\n",
    "num_actions = 100\n",
    "\n",
    "# feature dimension is 3\n",
    "dim = 3\n",
    "\n",
    "# time horizon is 1000\n",
    "horizon = 1000\n",
    "\n",
    "# run the simulations\n",
    "cum_regret = compareAgents(seed_vec, num_actions, dim, horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "wzxH9g22x2Pa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'cumulative regret')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "  ## generate the plot for the cumulative regrets\n",
    "  \n",
    "  time_vec = range(horizon)\n",
    "  plt.figure(1)\n",
    "  plt.clf()\n",
    "\n",
    "  plt.plot(time_vec, cum_regret['ats'], color='b', \n",
    "         linewidth=2, label='Approximate TS')\n",
    "\n",
    "  plt.plot(time_vec, cum_regret['epsilon-greedy'], color='g', \n",
    "         linewidth=2, label='epsilon-Greedy')\n",
    "\n",
    "  plt.legend(loc=\"best\", fontsize=9)\n",
    "  plt.xlabel(\"$t$\", fontsize=14)\n",
    "  plt.ylabel(\"cumulative regret\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuxK6CwGx72Z"
   },
   "source": [
    "## Problem 2: Contextual Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBJNuT9vx7_4"
   },
   "source": [
    "### Part a.1: the Contextual Recommendation Environment\n",
    "\n",
    "In this part, we implement the environment for the contextual recommendation problem.\n",
    "\n",
    "**You do not need to modify the code of this part.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RF2fid68xx6A"
   },
   "outputs": [],
   "source": [
    "class RecEnv:\n",
    "\n",
    "  # constructor\n",
    "  def __init__(self):\n",
    "\n",
    "    # set the attraction probabilities\n",
    "    self.attraction_probs = np.array([[0.3, 0.1, 0.25, 0.15], \n",
    "                                      [0.1, 0.3, 0.25, 0.15]])\n",
    "    \n",
    "    # the number of user types\n",
    "    self.num_user_type = 2\n",
    "\n",
    "    # the number of genres\n",
    "    self.num_genre = 4\n",
    "\n",
    "    # compute the instantaneous regret for each context-action pair\n",
    "    self.values = np.max(self.attraction_probs, axis=1, keepdims=True)\n",
    "    self.regret = self.values - self.attraction_probs\n",
    "    \n",
    "  def act(self, action, t):\n",
    "\n",
    "    # time t=0, when the action is non-essential\n",
    "    if t == 0:\n",
    "      reward = 0\n",
    "      inst_regret =0 \n",
    "\n",
    "    # time t>=1\n",
    "    else:\n",
    "      p = self.attraction_probs[self.user_type, action]\n",
    "      reward = np.random.binomial(1, p)\n",
    "      inst_regret = self.regret[self.user_type, action]\n",
    "\n",
    "    # sample user type (context) for next time step\n",
    "    self.user_type = np.random.randint(low=0, high=self.num_user_type)\n",
    "\n",
    "    return self.user_type, reward, inst_regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80D4O6icyFOA"
   },
   "source": [
    "### Part a.2: Agents\n",
    "\n",
    "In this part, we implement the Thompson sampling agents Agent_1 and Agent_2 as described in the problem. \n",
    "\n",
    "You need to add some code in the run function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zjW3SWRryB6d"
   },
   "outputs": [],
   "source": [
    "\"\"\"Agent_1: which uses the contextual information.\"\"\"\n",
    "\n",
    "class Agent_1:\n",
    "\n",
    "  # constructor\n",
    "  def __init__(self, env):\n",
    "\n",
    "    # set the environment\n",
    "    self.env = env\n",
    "\n",
    "    # number of user types\n",
    "    self.num_user_type = self.env.num_user_type\n",
    "\n",
    "    # number of genres\n",
    "    self.num_genre = self.env.num_genre\n",
    "\n",
    "  # call this function to run the experiments\n",
    "  # horizon is the time horizon\n",
    "  # num_sim is the number of simulations\n",
    "  def run(self, horizon, num_sim=1):\n",
    "\n",
    "    # placeholder for the results\n",
    "    reward_matrix = np.zeros((num_sim, horizon))\n",
    "    regret_matrix = np.zeros((num_sim, horizon))\n",
    "\n",
    "    for s in range(num_sim):\n",
    "\n",
    "      # initialize the alpha matrix and the beta matrix for Thompson sampling\n",
    "      alpha_matrix = np.ones((self.num_user_type, self.num_genre))\n",
    "      beta_matrix = np.ones((self.num_user_type, self.num_genre))\n",
    "\n",
    "      for t in range(horizon):\n",
    "\n",
    "        if t == 0:\n",
    "          # choose arbitrary action\n",
    "          action = 0\n",
    "          \n",
    "        else:\n",
    "          ############### Start of Code Modification ##########################\n",
    "          ### Delete \"pass\" and Write Code to choose action; \n",
    "          ### 2 to 4 lines of code expected \n",
    "          pass\n",
    "          ################### End of Code Modification ########################\n",
    "\n",
    "        next_context, reward, inst_regret = self.env.act(action, t=t)\n",
    "\n",
    "        reward_matrix[s, t] = reward\n",
    "        regret_matrix[s, t] = inst_regret\n",
    "\n",
    "        if t>0:\n",
    "          ############### Start of Code Modification ##########################\n",
    "          ### Delete \"pass\" and Write Code to update alpha_matrix and \n",
    "          ### beta_matrix based on context, action, and reward\n",
    "          ### 2 to 4 lines of code expected\n",
    "          pass\n",
    "          ################### End of Code Modification ########################\n",
    "\n",
    "        # update context as the next_context\n",
    "        context = next_context\n",
    "\n",
    "    # compute the mean cumulative reward and regret\n",
    "    cum_reward_mean = np.cumsum(np.mean(reward_matrix, axis=0))\n",
    "    cum_regret_mean = np.cumsum(np.mean(regret_matrix, axis=0))\n",
    "\n",
    "    return cum_reward_mean, cum_regret_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hy7-AYTpyHuB"
   },
   "outputs": [],
   "source": [
    "\"\"\"Agent_2: which ignores the contextual information\"\"\"\n",
    "\n",
    "class Agent_2:\n",
    "\n",
    "  # constructor\n",
    "  def __init__(self, env):\n",
    "\n",
    "    # set the environment\n",
    "    self.env = env\n",
    "\n",
    "    # number of genres\n",
    "    self.num_genre = self.env.num_genre\n",
    "\n",
    "  # call this function to run the experiments\n",
    "  # horizon is the time horizon\n",
    "  # num_sim is the number of simulations\n",
    "  def run(self, horizon, num_sim=1):\n",
    "\n",
    "    # placeholder for the results\n",
    "    reward_matrix = np.zeros((num_sim, horizon))\n",
    "    regret_matrix = np.zeros((num_sim, horizon))\n",
    "\n",
    "    for s in range(num_sim):\n",
    "\n",
    "      # initialize the alpha matrix and the beta matrix for Thompson sampling\n",
    "      alpha_vec = np.ones(self.num_genre)\n",
    "      beta_vec = np.ones(self.num_genre)\n",
    "\n",
    "      for t in range(horizon):\n",
    "\n",
    "        if t == 0:\n",
    "          # choose arbitrary action\n",
    "          action = 0\n",
    "        else:\n",
    "          ############### Start of Code Modification ##########################\n",
    "          ### Delete \"pass\" and Write Code to choose action; \n",
    "          ### 2 to 4 lines of code expected \n",
    "          pass\n",
    "          ################### End of Code Modification ########################\n",
    "\n",
    "        # apply action, and observe the reward\n",
    "        _, reward, inst_regret = self.env.act(action, t=t)\n",
    "          \n",
    "        reward_matrix[s, t] = reward\n",
    "        regret_matrix[s, t] = inst_regret\n",
    "\n",
    "        if t>0:\n",
    "          ############### Start of Code Modification ##########################\n",
    "          ### Delete \"pass\" and Write Code to update alpha_vec and beta_vec  \n",
    "          ### based on action, and reward\n",
    "          ### 2 to 4 lines of code expected\n",
    "          pass\n",
    "          ################### End of Code Modification ########################\n",
    "\n",
    "    # compute the mean cumulative reward and regret\n",
    "    cum_reward_mean = np.cumsum(np.mean(reward_matrix, axis=0))\n",
    "    cum_regret_mean = np.cumsum(np.mean(regret_matrix, axis=0))\n",
    "    return cum_reward_mean, cum_regret_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ln0T5bnpydYO"
   },
   "source": [
    "### Part a.3: Show Results\n",
    "\n",
    "You do not need to modify the code below. You just need to run them to generate plots for cumulative rewards and cumulative regrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qx1_rJzVyXoT"
   },
   "outputs": [],
   "source": [
    "# set up the environment and the agent\n",
    "env = RecEnv()\n",
    "agent_1 = Agent_1(env)\n",
    "agent_2 = Agent_2(env)\n",
    "\n",
    "# number of simulations\n",
    "num_sim = 100\n",
    "\n",
    "# time horizon\n",
    "horizon = 5000\n",
    "\n",
    "# run agent_1\n",
    "cum_reward_agent_1, cum_regret_agent_1 = agent_1.run(horizon=horizon, num_sim=num_sim)\n",
    "\n",
    "# run agent_2\n",
    "cum_reward_agent_2, cum_regret_agent_2 = agent_2.run(horizon=horizon, num_sim=num_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SuDHZ876yfr2"
   },
   "outputs": [],
   "source": [
    "# plot the cumulative reward\n",
    "\n",
    "plt.figure(2)\n",
    "t_vec = range(horizon)\n",
    "\n",
    "plt.plot(t_vec, cum_reward_agent_1, color='b', linewidth=2, label='agent 1')\n",
    "plt.plot(t_vec, cum_reward_agent_2, color='r', linewidth=2, label='agent 2')\n",
    "\n",
    "plt.legend(loc=\"best\", fontsize=9)\n",
    "plt.xlabel(\"$t$\", fontsize=14)\n",
    "plt.ylabel(\"cumulative reward\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfutU__VytLK"
   },
   "outputs": [],
   "source": [
    "# plot the cumulative regret\n",
    "\n",
    "plt.figure(3)\n",
    "t_vec = range(horizon)\n",
    "\n",
    "plt.plot(t_vec, cum_regret_agent_1, color='b', linewidth=2, label='agent 1')\n",
    "plt.plot(t_vec, cum_regret_agent_2, color='r', linewidth=2, label='agent 2')\n",
    "\n",
    "plt.legend(loc=\"best\", fontsize=9)\n",
    "plt.xlabel(\"$t$\", fontsize=14)\n",
    "plt.ylabel(\"cumulative regret\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUA1QpwEyzbI"
   },
   "source": [
    "## Problem 3: Features for Logistic bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_8_p0eDz9PD"
   },
   "source": [
    "## Problem 4: Online Crowdsourcing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzCXblOG0FNf"
   },
   "source": [
    "### Part c.1: the Online Crowdsourcing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "B4zKeLCyyy2r"
   },
   "outputs": [],
   "source": [
    "#@title Crowdsourcing environment\n",
    "\n",
    "class CrowdsourcingEnv:\n",
    "\n",
    "  def __init__(self, M: int, L: int, q: float):\n",
    "    \"\"\"Constructor.\"\"\"\n",
    "\n",
    "    assert L < M\n",
    "    self.M = M\n",
    "    self.L = L\n",
    "\n",
    "    self.q = q\n",
    "\n",
    "    self.p = np.random.beta(1, 1, M)\n",
    "\n",
    "    assert len(self.p) == M\n",
    "\n",
    "  def get_question(self):\n",
    "    \"\"\"get a question.\"\"\"\n",
    "\n",
    "    if np.random.binomial(1, self.q) == 1:\n",
    "      self.correct_answer = 'Yes'\n",
    "      self.wrong_answer = 'No'\n",
    "    else:\n",
    "      self.correct_answer = 'No'\n",
    "      self.wrong_answer = 'Yes'\n",
    "\n",
    "\n",
    "  def get_answers(self, selected_participants):\n",
    "    \"\"\"get participants' answers, where selected_participants is a list of integers, and each integer corresponds to a participant.\"\"\"\n",
    "\n",
    "    assert len(selected_participants) == self.L\n",
    "    answers = {}\n",
    "\n",
    "    for participant in selected_participants:\n",
    "\n",
    "      # ensure that the participant is an integer\n",
    "      participant = int(participant)\n",
    "\n",
    "      # whether or not the participant's answer is correct\n",
    "      correct_prob = self.p[participant]\n",
    "      correct = np.random.binomial(1, correct_prob)\n",
    "\n",
    "      # append the participant's answer to answers\n",
    "      if correct:\n",
    "        answers[participant] = self.correct_answer\n",
    "      else:\n",
    "        answers[participant] = self.wrong_answer\n",
    "\n",
    "    return answers\n",
    "\n",
    "  def get_correct_answer_and_reward(self, answer):\n",
    "    \"\"\"get correct answer and reward based on the agent's answer.\"\"\"\n",
    "\n",
    "    if answer == self.correct_answer:\n",
    "      return self.correct_answer, 1\n",
    "    else:\n",
    "      return self.correct_answer, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gREaypRF1flO"
   },
   "source": [
    "### Part c.2: the Thompson sampling agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "zwEBRv3Y0D_R"
   },
   "outputs": [],
   "source": [
    "#@title Thompson sampling agent for online crowdsourcing\n",
    "\n",
    "class TS4CrowdSourcing:\n",
    "\n",
    "  def __init__(self, env):\n",
    "\n",
    "    self.env = env\n",
    "\n",
    "    self.M = env.M\n",
    "    self.L = env.L\n",
    "\n",
    "    self.q = env.q\n",
    "\n",
    "  # call this function to run the experiments\n",
    "  # horizon is the time horizon\n",
    "  # num_sim is the number of simulations\n",
    "  def run(self, horizon, num_sim=1):\n",
    "\n",
    "    # placeholder for the results\n",
    "    reward_matrix = np.zeros((num_sim, horizon))\n",
    "\n",
    "    for s in range(num_sim):\n",
    "\n",
    "      # initialize the alpha matrix and the beta matrix for Thompson sampling\n",
    "      alpha_vec = np.ones(self.M)\n",
    "      beta_vec = np.ones(self.M)\n",
    "\n",
    "      for t in range(horizon):\n",
    "\n",
    "        # sample a vector of p's\n",
    "        p_vec = np.random.beta(alpha_vec, beta_vec)\n",
    "\n",
    "        self.env.get_question()\n",
    "\n",
    "        ############### Start of Code Modification ##########################\n",
    "        ### Write Code to select L participants; 2 to 4 lines of code expected\n",
    "        selected_participants = \n",
    "        ################### End of Code Modification ########################\n",
    "\n",
    "        participant_answers = self.env.get_answers(selected_participants)\n",
    "\n",
    "        ############### Start of Code Modification ##########################\n",
    "        ### Write Code to select the agent's answer based on participants' answers; \n",
    "        ### 8-15 lines of code expected\n",
    "        agent_answer = \n",
    "        ################### End of Code Modification ########################\n",
    "\n",
    "        correct_answer, reward = self.env.get_correct_answer_and_reward(agent_answer)\n",
    "\n",
    "        # update posterior\n",
    "        for i in range(self.L):\n",
    "          participant = selected_participants[i]\n",
    "\n",
    "          if participant_answers[participant] == correct_answer:\n",
    "            alpha_vec[participant] += 1\n",
    "          else:\n",
    "            beta_vec[participant] += 1\n",
    "\n",
    "        reward_matrix[s,t] = reward\n",
    "\n",
    "    # compute the mean per-step reward \n",
    "    reward_mean = np.mean(reward_matrix, axis=0)\n",
    "    return reward_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4bY8Bx31ssV"
   },
   "source": [
    "### Part c.3: Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcNmNjBk1ohn"
   },
   "outputs": [],
   "source": [
    "#@title Experiment setup\n",
    "\n",
    "M = 20\n",
    "L = 3\n",
    "\n",
    "q = 0.55\n",
    "\n",
    "horizon = 1000\n",
    "num_sim = 500\n",
    "\n",
    "env = CrowdsourcingEnv(M, L, q)\n",
    "agent = TS4CrowdSourcing(env)\n",
    "reward_mean = agent.run(horizon, num_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPQTPLmo2SQJ"
   },
   "outputs": [],
   "source": [
    "#@title Plot the results\n",
    "\n",
    "plt.plot(range(horizon), reward_mean, linewidth=2)\n",
    "\n",
    "plt.xlabel(\"$t$\", fontsize=14)\n",
    "plt.ylabel(\"per-step reward\", fontsize=14)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
