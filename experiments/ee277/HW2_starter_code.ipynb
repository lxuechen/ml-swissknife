{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WC4dLTtnxD0I"
   },
   "source": [
    "# Template for Homework 2, EE 277"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "YTJniRDFw6S-"
   },
   "outputs": [],
   "source": [
    "#@title imports \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_67yA44BxRFm"
   },
   "source": [
    "## Problem 1: XOR Logistic Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwmWS5i3xV5C"
   },
   "source": [
    "### Part 0: XOR Logistic Bandit Environment\n",
    "\n",
    "In this part, we implement the XOR function and the XOR logistic bandit environment.\n",
    "\n",
    "**You do not need to modify the code in this part.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "98EX_uAexVrM"
   },
   "outputs": [],
   "source": [
    "\"\"\" XOR function\"\"\"\n",
    "\n",
    "def XOR(x, y):\n",
    "  if (x == 0 and y == 1) or (x==1 and y==0):\n",
    "    return 1\n",
    "  else:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "l2-i26HAxHwc"
   },
   "outputs": [],
   "source": [
    "\"\"\" The environment for XOR logistic bandit\"\"\"\n",
    "\n",
    "class XORLogisticBandit:\n",
    "\n",
    "  \"\"\"\n",
    "  The constructor samples an XOR logistic bandit environment\n",
    "  where\n",
    "  1) num_actions is the number of actions (K in the problem statement)\n",
    "  2) dim is the feature dimension (N in the problem statement)\n",
    "  3) the prior distribution for theta is N(0, sigma^2 I)\n",
    "  \"\"\"\n",
    "  def __init__(self, num_actions, dim, sigma_p):\n",
    "\n",
    "    # number of actions\n",
    "    self.num_actions = num_actions\n",
    "\n",
    "    # feature dimension\n",
    "    self.dim = dim\n",
    "\n",
    "    # sample theta and generate environment\n",
    "    self.sigma_p = sigma_p\n",
    "    self.sigma_p_squared = sigma_p**2\n",
    "\n",
    "    # theta (to be learned)\n",
    "    self.theta = sigma_p * np.random.randn(dim)\n",
    "\n",
    "    # feature matrix, feature.shape=(num_actions, dim)\n",
    "    feature = np.random.randn(num_actions, dim)\n",
    "    self.feature = feature/np.linalg.norm(feature, axis=-1, keepdims=True)\n",
    "\n",
    "    exp_logits = np.exp(np.tensordot(self.feature, \\\n",
    "                                     self.theta, axes=([-1], [0])))\n",
    "\n",
    "    # expected reward for all context-action pairs\n",
    "    # self.rewards.shape = (num_actions)\n",
    "    self.p = exp_logits/(1 + exp_logits)\n",
    "    self.rewards = 2 * np.multiply(self.p, 1-self.p)\n",
    "\n",
    "    # values (optimal reward at each context)\n",
    "    # self.values.shape = (1)\n",
    "    self.values = np.max(self.rewards, axis=0, keepdims=True)\n",
    "\n",
    "    # expected instantanesou regret for all context-action pairs\n",
    "    # self.regrets.shape = (num_actions)\n",
    "    self.regrets = self.values - self.rewards\n",
    "\n",
    "  # return the feature matrix of the environment\n",
    "  def get_feature(self):\n",
    "    return self.feature\n",
    "\n",
    "  # agents choose actions by calling this method\n",
    "  def act(self, action):\n",
    "\n",
    "    # assert that action is in the right range\n",
    "    assert action >= 0 and action < self.num_actions\n",
    "\n",
    "    # the instantaneous regret \n",
    "    inst_regret = self.regrets[action]\n",
    "\n",
    "    # probability of observing 1\n",
    "    p_1 = self.p[action]\n",
    "    obs = (np.random.binomial(1, p_1), np.random.binomial(1, p_1))\n",
    "\n",
    "    # reward, which is XOR of the two observations\n",
    "    reward = XOR(obs[0], obs[1])\n",
    "\n",
    "    # returns observation, realized reward, and inst_regret\n",
    "    return obs, reward, inst_regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhjpc3kfxdv-"
   },
   "source": [
    "### Part a: Epsilon-Greedy Agent\n",
    "\n",
    "**You need to implement the select_action method for epsilon-greedy agent.**\n",
    "\n",
    "**Technical Issue:** In case you are interested, we use LogisticRegression module from sklearn to solve the logistic regression. LogisticRegression module requires data with both observation 0 and observation 1. Thus, the agent will start solving logistic regressions once it has seen at least one obervation 0 and one observation 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IpjOT0mrxa_8"
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedyAgent:\n",
    "\n",
    "  # environment is an XORLogisticBandit\n",
    "  def __init__(self, env):\n",
    "    self.env = env\n",
    "    self.num_actions = self.env.num_actions\n",
    "    self.dim = self.env.dim\n",
    "    self.sigma_p_squared = self.env.sigma_p_squared\n",
    "\n",
    "    # the logistic regression solver\n",
    "    # notice that the choice of L2 penelty and C adds a quadratic regularization\n",
    "    # term corresponding to the prior\n",
    "    # we do not need an intercept in the logistic regression model\n",
    "    # warm_start will use the previous solution as the starting point, which \n",
    "    # will speed up learning\n",
    "    self.solver = LogisticRegression(penalty='l2', \n",
    "                                     C=(2*self.sigma_p_squared),\n",
    "                                     fit_intercept=False,\n",
    "                                     warm_start=True)\n",
    "    \n",
    "\n",
    "  ## theta_est: the current point estimate of theta\n",
    "  ## theta_est.shape = (dim)\n",
    "  ## epsilon is the epsilon in epsilon-greedy algorithms\n",
    "  ## phi is a matrix encoding the action features\n",
    "  ## phi.shape = (num_actions, dim)\n",
    "  def select_action(self, theta_est, epsilon, phi):\n",
    "    ############### Start of Code Modification ################################\n",
    "    ### Please implement the action selection strategy for epsilon-greedy agent\n",
    "    ###########################################################################\n",
    "    if np.random.rand() < epsilon:  # explore\n",
    "        return np.random.randint(low=0, high=self.num_actions)\n",
    "    else:  # exploit\n",
    "        action_potentials = np.abs(phi @ theta_est)\n",
    "        return np.argmax(action_potentials)\n",
    "    ################### End of Code Modification ##############################\n",
    "  \n",
    "  def run(self, horizon=50, epsilon=0.05, verbose=False):\n",
    "    \"\"\"run simulation.\"\"\"\n",
    "\n",
    "    # placeholder for the results\n",
    "    regret_vec = np.zeros(horizon)\n",
    "    cum_regret = 0\n",
    "\n",
    "    # placeholder for used features and observed rewards\n",
    "    X = np.zeros((2*horizon, self.dim))\n",
    "    y = np.zeros(2*horizon)\n",
    "\n",
    "    # initial estimated theta\n",
    "    theta_est = np.zeros(self.dim)\n",
    "\n",
    "    # initialization_phase will be false once we observe both \n",
    "    # an observation 0 and an observation 1\n",
    "    initialization_phase = True\n",
    "\n",
    "    # get features\n",
    "    phi = self.env.get_feature()\n",
    "\n",
    "    for t in range(horizon):\n",
    "\n",
    "      # select action based on epsilon-greedy\n",
    "      action = self.select_action(theta_est, epsilon, phi)\n",
    "\n",
    "      # apply the chosen action\n",
    "      obs, reward, inst_regret  = self.env.act(action)\n",
    "\n",
    "      # update regret\n",
    "      regret_vec[t] = inst_regret\n",
    "      cum_regret += inst_regret\n",
    "\n",
    "      # update the placeholder\n",
    "      X[2*t,:] = phi[action, :]; X[2*t+1,:] = phi[action, :];\n",
    "      y[2*t] = obs[0]; y[2*t+1] = obs[1]\n",
    "\n",
    "      if verbose and (t+1)%100==0:\n",
    "        print(\"Epsilon-Greedy Agent: the cum regret by t={} is {}\".format(t, cum_regret))\n",
    "\n",
    "      # check if we have observations with both reward 0 and 1\n",
    "      if initialization_phase and len(np.unique(y[:(t+1)])) == 2:\n",
    "        initialization_phase = False\n",
    "        if verbose:\n",
    "          print(\"Initialization Phase ends in time step {}\".format(t))\n",
    "        \n",
    "      if not initialization_phase:\n",
    "        # run logistic regression and obtain the learned theta\n",
    "        self.solver.fit(X[:(2*t+2),:], y[:(2*t+2)])\n",
    "        theta_est = self.solver.coef_.reshape(self.dim)\n",
    "\n",
    "    return regret_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4SMjDvZxkt0"
   },
   "source": [
    "### Part b: Approximate Thompson Sampling Agent Based on Laplace Approximation\n",
    "\n",
    "**You need to implement the select_action method for Gaussian approximate Thompson sampling agent.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kqR5PFJjxf3z"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function computes the inverse Hessian matrix of the loss function\n",
    "theta.shape = (dim)\n",
    "X.shape = (num_samples, dim)\n",
    "'''\n",
    "def getInvHessian(theta, X, sigma_p_squared):\n",
    "\n",
    "  H = np.eye(len(theta))/sigma_p_squared\n",
    "\n",
    "  if X.shape[0] > 0:\n",
    "    logits = np.dot(X, theta)\n",
    "    tmp = np.exp(logits); y_pred = np.divide(tmp, 1+tmp)\n",
    "    D = np.diag(np.multiply(y_pred, 1-y_pred))\n",
    "\n",
    "    H += np.dot(np.dot(X.T, D), X) \n",
    "\n",
    "  return np.linalg.inv(H)\n",
    "\n",
    "\n",
    "class ApproximateTSAgent:\n",
    "\n",
    "  # environment is an XORLogisticBandit\n",
    "  def __init__(self, env):\n",
    "    self.env = env\n",
    "    self.num_actions = self.env.num_actions\n",
    "    self.dim = self.env.dim\n",
    "    self.sigma_p_squared = self.env.sigma_p_squared\n",
    "\n",
    "    # the logistic regression solver\n",
    "    # notice that the choice of L2 penelty and C adds a quadratic regularization\n",
    "    # term corresponding to the prior\n",
    "    # we do not need an intercept in the logistic regression model\n",
    "    # warm_start will use the previous solution as the starting point, which \n",
    "    # will speed up learning\n",
    "    self.solver = LogisticRegression(penalty='l2', \n",
    "                                     C=(2*self.sigma_p_squared),\n",
    "                                     fit_intercept=False,\n",
    "                                     warm_start=True)\n",
    "    \n",
    "\n",
    "  ## mu is the mean of Gaussian approximation\n",
    "  ## mu.shape = (dim)\n",
    "  ## Sigma is the covariance matrix of Gaussian approximation\n",
    "  ## Sigma.shape = (dim, dim)\n",
    "  ## phi is a matrix encoding the action features\n",
    "  ## phi.shape = (num_actions, dim)\n",
    "  def select_action(self, mu, Sigma, phi):\n",
    "    ############### Start of Code Modification ################################\n",
    "    ### Please implement the action selection strategy for approximate TS\n",
    "    ### agent\n",
    "    ###########################################################################\n",
    "    theta = np.random.multivariate_normal(mu, Sigma)\n",
    "    action_potentials = np.abs(phi @ theta)\n",
    "    return np.argmax(action_potentials)\n",
    "    ################### End of Code Modification ##############################\n",
    "  \n",
    "  def run(self, horizon=50, verbose=False):\n",
    "    \"\"\"run simulation.\"\"\"\n",
    "\n",
    "    # placeholder for the results\n",
    "    regret_vec = np.zeros(horizon)\n",
    "    cum_regret = 0\n",
    "\n",
    "    # placeholder for used features and observed rewards\n",
    "    X = np.zeros((2*horizon, self.dim))\n",
    "    y = np.zeros(2*horizon)\n",
    "\n",
    "    # initial mu and Sigma\n",
    "    mu = np.zeros(self.dim)\n",
    "    Sigma = self.sigma_p_squared*np.eye(self.dim)\n",
    "\n",
    "    # initialization_phase will be false once we observe both a reward 0 and\n",
    "    # a reward 1\n",
    "    initialization_phase = True\n",
    "\n",
    "    # get features\n",
    "    phi = self.env.get_feature()\n",
    "\n",
    "    for t in range(horizon):\n",
    "\n",
    "      # select action based on epsilon-greedy\n",
    "      action = self.select_action(mu, Sigma, phi)\n",
    "\n",
    "      # apply the chosen action\n",
    "      obs, reward, inst_regret  = self.env.act(action)\n",
    "\n",
    "      # update regret\n",
    "      regret_vec[t] = inst_regret\n",
    "      cum_regret += inst_regret\n",
    "\n",
    "      # update the placeholder\n",
    "      X[2*t,:] = phi[action, :]; X[2*t+1,:] = phi[action, :];\n",
    "      y[2*t] = obs[0]; y[2*t+1] = obs[1]\n",
    "\n",
    "      if verbose and (t+1)%100==0:\n",
    "        print(\"Laplace Agent: the cum regret by t={} is {}\".format(t, cum_regret))\n",
    "\n",
    "      # check if we have observations with both reward 0 and 1\n",
    "      if initialization_phase and len(np.unique(y[:(t+1)])) == 2:\n",
    "        initialization_phase = False\n",
    "        if verbose:\n",
    "          print(\"Initialization Phase ends in time step {}\".format(t))\n",
    "        \n",
    "      if not initialization_phase:\n",
    "        # run logistic regression and obtain the learned theta\n",
    "        self.solver.fit(X[:(2*t+2),:], y[:(2*t+2)])\n",
    "        mu = self.solver.coef_.reshape(self.dim)\n",
    "\n",
    "        # compute the covariance matrix\n",
    "        Sigma = getInvHessian(mu, X[:(2*t+2),:], self.sigma_p_squared)\n",
    "\n",
    "    return regret_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJzdI6VbxrrR"
   },
   "source": [
    "### Part d: Show Results\n",
    "\n",
    "**You do not need to modify the code below. You just need to run it to generate the plot for the cumulative regrets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xyLh5l8oxnUv"
   },
   "outputs": [],
   "source": [
    "# function to run the experiments\n",
    "def compareAgents(seed_vec, num_actions, dim, horizon, sigma_p=1):\n",
    "\n",
    "  # placeholders for the results\n",
    "  regret_matrix_eps_greedy = np.zeros((len(seed_vec), horizon))\n",
    "  regret_matrix_laplace = np.zeros((len(seed_vec), horizon))\n",
    "\n",
    "  # loop over the random seeds\n",
    "  for i in range(len(seed_vec)):\n",
    "\n",
    "    np.random.seed(seed_vec[i])\n",
    "    print('For seed={} with horizon {}, the total regrets are:'.format(seed_vec[i], horizon))\n",
    "\n",
    "    # randomly sample one environment\n",
    "    env = XORLogisticBandit(num_actions = num_actions,\n",
    "                        dim = dim, sigma_p = sigma_p)\n",
    "    \n",
    "    # epsilon-greedy agent\n",
    "    agent_epsilon = EpsilonGreedyAgent(env)\n",
    "    regret_matrix_eps_greedy[i,:] = agent_epsilon.run(horizon=horizon, epsilon=0.05)\n",
    "    print('Epsilon-Greedy: {}'.format(np.sum(regret_matrix_eps_greedy[i, :])))\n",
    "\n",
    "    # approximate Thompson sampling agent\n",
    "    agent_laplace = ApproximateTSAgent(env)\n",
    "    regret_matrix_laplace[i,:] = agent_laplace.run(horizon=horizon)\n",
    "    print('Approximate TS: {}'.format(np.sum(regret_matrix_laplace[i, :])))\n",
    "\n",
    "  # compute the mean cumulative regrets\n",
    "  cum_regret = {}\n",
    "  cum_regret['epsilon-greedy'] = np.cumsum(np.mean(regret_matrix_eps_greedy, axis=0))\n",
    "  cum_regret['ats'] = np.cumsum(np.mean(regret_matrix_laplace, axis=0))\n",
    "\n",
    "  return cum_regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LN5XH8M-xuxX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For seed=0 with horizon 1000, the total regrets are:\n",
      "Epsilon-Greedy: 281.3604197702397\n",
      "Approximate TS: 280.4577758502637\n",
      "For seed=1 with horizon 1000, the total regrets are:\n",
      "Epsilon-Greedy: 223.4888734552955\n",
      "Approximate TS: 230.09688515821023\n",
      "For seed=2 with horizon 1000, the total regrets are:\n",
      "Epsilon-Greedy: 297.02338307642435\n"
     ]
    }
   ],
   "source": [
    "## Experiment set up\n",
    "\n",
    "# 10 simulations, with random seed = 0,1, ... 9\n",
    "seed_vec = range(10)\n",
    "\n",
    "# 100 actions\n",
    "num_actions = 100\n",
    "\n",
    "# feature dimension is 3\n",
    "dim = 3\n",
    "\n",
    "# time horizon is 1000\n",
    "horizon = 1000\n",
    "\n",
    "# run the simulations\n",
    "cum_regret = compareAgents(seed_vec, num_actions, dim, horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wzxH9g22x2Pa"
   },
   "outputs": [],
   "source": [
    "  ## generate the plot for the cumulative regrets\n",
    "  \n",
    "  time_vec = range(horizon)\n",
    "  plt.figure(1)\n",
    "  plt.clf()\n",
    "\n",
    "  plt.plot(time_vec, cum_regret['ats'], color='b', \n",
    "         linewidth=2, label='Approximate TS')\n",
    "\n",
    "  plt.plot(time_vec, cum_regret['epsilon-greedy'], color='g', \n",
    "         linewidth=2, label='epsilon-Greedy')\n",
    "\n",
    "  plt.legend(loc=\"best\", fontsize=9)\n",
    "  plt.xlabel(\"$t$\", fontsize=14)\n",
    "  plt.ylabel(\"cumulative regret\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuxK6CwGx72Z"
   },
   "source": [
    "## Problem 2: Contextual Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBJNuT9vx7_4"
   },
   "source": [
    "### Part a.1: the Contextual Recommendation Environment\n",
    "\n",
    "In this part, we implement the environment for the contextual recommendation problem.\n",
    "\n",
    "**You do not need to modify the code of this part.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RF2fid68xx6A"
   },
   "outputs": [],
   "source": [
    "class RecEnv:\n",
    "\n",
    "  # constructor\n",
    "  def __init__(self):\n",
    "\n",
    "    # set the attraction probabilities\n",
    "    self.attraction_probs = np.array([[0.3, 0.1, 0.25, 0.15], \n",
    "                                      [0.1, 0.3, 0.25, 0.15]])\n",
    "    \n",
    "    # the number of user types\n",
    "    self.num_user_type = 2\n",
    "\n",
    "    # the number of genres\n",
    "    self.num_genre = 4\n",
    "\n",
    "    # compute the instantaneous regret for each context-action pair\n",
    "    self.values = np.max(self.attraction_probs, axis=1, keepdims=True)\n",
    "    self.regret = self.values - self.attraction_probs\n",
    "    \n",
    "  def act(self, action, t):\n",
    "\n",
    "    # time t=0, when the action is non-essential\n",
    "    if t == 0:\n",
    "      reward = 0\n",
    "      inst_regret =0 \n",
    "\n",
    "    # time t>=1\n",
    "    else:\n",
    "      p = self.attraction_probs[self.user_type, action]\n",
    "      reward = np.random.binomial(1, p)\n",
    "      inst_regret = self.regret[self.user_type, action]\n",
    "\n",
    "    # sample user type (context) for next time step\n",
    "    self.user_type = np.random.randint(low=0, high=self.num_user_type)\n",
    "\n",
    "    return self.user_type, reward, inst_regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80D4O6icyFOA"
   },
   "source": [
    "### Part a.2: Agents\n",
    "\n",
    "In this part, we implement the Thompson sampling agents Agent_1 and Agent_2 as described in the problem. \n",
    "\n",
    "You need to add some code in the run function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zjW3SWRryB6d"
   },
   "outputs": [],
   "source": [
    "\"\"\"Agent_1: which uses the contextual information.\"\"\"\n",
    "\n",
    "class Agent_1:\n",
    "\n",
    "  # constructor\n",
    "  def __init__(self, env):\n",
    "\n",
    "    # set the environment\n",
    "    self.env = env\n",
    "\n",
    "    # number of user types\n",
    "    self.num_user_type = self.env.num_user_type\n",
    "\n",
    "    # number of genres\n",
    "    self.num_genre = self.env.num_genre\n",
    "\n",
    "  # call this function to run the experiments\n",
    "  # horizon is the time horizon\n",
    "  # num_sim is the number of simulations\n",
    "  def run(self, horizon, num_sim=1):\n",
    "\n",
    "    # placeholder for the results\n",
    "    reward_matrix = np.zeros((num_sim, horizon))\n",
    "    regret_matrix = np.zeros((num_sim, horizon))\n",
    "\n",
    "    for s in range(num_sim):\n",
    "\n",
    "      # initialize the alpha matrix and the beta matrix for Thompson sampling\n",
    "      alpha_matrix = np.ones((self.num_user_type, self.num_genre))\n",
    "      beta_matrix = np.ones((self.num_user_type, self.num_genre))\n",
    "\n",
    "      for t in range(horizon):\n",
    "\n",
    "        if t == 0:\n",
    "          # choose arbitrary action\n",
    "          action = 0\n",
    "          \n",
    "        else:\n",
    "          ############### Start of Code Modification ##########################\n",
    "          ### Delete \"pass\" and Write Code to choose action; \n",
    "          ### 2 to 4 lines of code expected \n",
    "          pass\n",
    "          ################### End of Code Modification ########################\n",
    "\n",
    "        next_context, reward, inst_regret = self.env.act(action, t=t)\n",
    "\n",
    "        reward_matrix[s, t] = reward\n",
    "        regret_matrix[s, t] = inst_regret\n",
    "\n",
    "        if t>0:\n",
    "          ############### Start of Code Modification ##########################\n",
    "          ### Delete \"pass\" and Write Code to update alpha_matrix and \n",
    "          ### beta_matrix based on context, action, and reward\n",
    "          ### 2 to 4 lines of code expected\n",
    "          pass\n",
    "          ################### End of Code Modification ########################\n",
    "\n",
    "        # update context as the next_context\n",
    "        context = next_context\n",
    "\n",
    "    # compute the mean cumulative reward and regret\n",
    "    cum_reward_mean = np.cumsum(np.mean(reward_matrix, axis=0))\n",
    "    cum_regret_mean = np.cumsum(np.mean(regret_matrix, axis=0))\n",
    "\n",
    "    return cum_reward_mean, cum_regret_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hy7-AYTpyHuB"
   },
   "outputs": [],
   "source": [
    "\"\"\"Agent_2: which ignores the contextual information\"\"\"\n",
    "\n",
    "class Agent_2:\n",
    "\n",
    "  # constructor\n",
    "  def __init__(self, env):\n",
    "\n",
    "    # set the environment\n",
    "    self.env = env\n",
    "\n",
    "    # number of genres\n",
    "    self.num_genre = self.env.num_genre\n",
    "\n",
    "  # call this function to run the experiments\n",
    "  # horizon is the time horizon\n",
    "  # num_sim is the number of simulations\n",
    "  def run(self, horizon, num_sim=1):\n",
    "\n",
    "    # placeholder for the results\n",
    "    reward_matrix = np.zeros((num_sim, horizon))\n",
    "    regret_matrix = np.zeros((num_sim, horizon))\n",
    "\n",
    "    for s in range(num_sim):\n",
    "\n",
    "      # initialize the alpha matrix and the beta matrix for Thompson sampling\n",
    "      alpha_vec = np.ones(self.num_genre)\n",
    "      beta_vec = np.ones(self.num_genre)\n",
    "\n",
    "      for t in range(horizon):\n",
    "\n",
    "        if t == 0:\n",
    "          # choose arbitrary action\n",
    "          action = 0\n",
    "        else:\n",
    "          ############### Start of Code Modification ##########################\n",
    "          ### Delete \"pass\" and Write Code to choose action; \n",
    "          ### 2 to 4 lines of code expected \n",
    "          pass\n",
    "          ################### End of Code Modification ########################\n",
    "\n",
    "        # apply action, and observe the reward\n",
    "        _, reward, inst_regret = self.env.act(action, t=t)\n",
    "          \n",
    "        reward_matrix[s, t] = reward\n",
    "        regret_matrix[s, t] = inst_regret\n",
    "\n",
    "        if t>0:\n",
    "          ############### Start of Code Modification ##########################\n",
    "          ### Delete \"pass\" and Write Code to update alpha_vec and beta_vec  \n",
    "          ### based on action, and reward\n",
    "          ### 2 to 4 lines of code expected\n",
    "          pass\n",
    "          ################### End of Code Modification ########################\n",
    "\n",
    "    # compute the mean cumulative reward and regret\n",
    "    cum_reward_mean = np.cumsum(np.mean(reward_matrix, axis=0))\n",
    "    cum_regret_mean = np.cumsum(np.mean(regret_matrix, axis=0))\n",
    "    return cum_reward_mean, cum_regret_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ln0T5bnpydYO"
   },
   "source": [
    "### Part a.3: Show Results\n",
    "\n",
    "You do not need to modify the code below. You just need to run them to generate plots for cumulative rewards and cumulative regrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qx1_rJzVyXoT"
   },
   "outputs": [],
   "source": [
    "# set up the environment and the agent\n",
    "env = RecEnv()\n",
    "agent_1 = Agent_1(env)\n",
    "agent_2 = Agent_2(env)\n",
    "\n",
    "# number of simulations\n",
    "num_sim = 100\n",
    "\n",
    "# time horizon\n",
    "horizon = 5000\n",
    "\n",
    "# run agent_1\n",
    "cum_reward_agent_1, cum_regret_agent_1 = agent_1.run(horizon=horizon, num_sim=num_sim)\n",
    "\n",
    "# run agent_2\n",
    "cum_reward_agent_2, cum_regret_agent_2 = agent_2.run(horizon=horizon, num_sim=num_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SuDHZ876yfr2"
   },
   "outputs": [],
   "source": [
    "# plot the cumulative reward\n",
    "\n",
    "plt.figure(2)\n",
    "t_vec = range(horizon)\n",
    "\n",
    "plt.plot(t_vec, cum_reward_agent_1, color='b', linewidth=2, label='agent 1')\n",
    "plt.plot(t_vec, cum_reward_agent_2, color='r', linewidth=2, label='agent 2')\n",
    "\n",
    "plt.legend(loc=\"best\", fontsize=9)\n",
    "plt.xlabel(\"$t$\", fontsize=14)\n",
    "plt.ylabel(\"cumulative reward\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfutU__VytLK"
   },
   "outputs": [],
   "source": [
    "# plot the cumulative regret\n",
    "\n",
    "plt.figure(3)\n",
    "t_vec = range(horizon)\n",
    "\n",
    "plt.plot(t_vec, cum_regret_agent_1, color='b', linewidth=2, label='agent 1')\n",
    "plt.plot(t_vec, cum_regret_agent_2, color='r', linewidth=2, label='agent 2')\n",
    "\n",
    "plt.legend(loc=\"best\", fontsize=9)\n",
    "plt.xlabel(\"$t$\", fontsize=14)\n",
    "plt.ylabel(\"cumulative regret\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUA1QpwEyzbI"
   },
   "source": [
    "## Problem 3: Features for Logistic bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_8_p0eDz9PD"
   },
   "source": [
    "## Problem 4: Online Crowdsourcing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzCXblOG0FNf"
   },
   "source": [
    "### Part c.1: the Online Crowdsourcing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "B4zKeLCyyy2r"
   },
   "outputs": [],
   "source": [
    "#@title Crowdsourcing environment\n",
    "\n",
    "class CrowdsourcingEnv:\n",
    "\n",
    "  def __init__(self, M: int, L: int, q: float):\n",
    "    \"\"\"Constructor.\"\"\"\n",
    "\n",
    "    assert L < M\n",
    "    self.M = M\n",
    "    self.L = L\n",
    "\n",
    "    self.q = q\n",
    "\n",
    "    self.p = np.random.beta(1, 1, M)\n",
    "\n",
    "    assert len(self.p) == M\n",
    "\n",
    "  def get_question(self):\n",
    "    \"\"\"get a question.\"\"\"\n",
    "\n",
    "    if np.random.binomial(1, self.q) == 1:\n",
    "      self.correct_answer = 'Yes'\n",
    "      self.wrong_answer = 'No'\n",
    "    else:\n",
    "      self.correct_answer = 'No'\n",
    "      self.wrong_answer = 'Yes'\n",
    "\n",
    "\n",
    "  def get_answers(self, selected_participants):\n",
    "    \"\"\"get participants' answers, where selected_participants is a list of integers, and each integer corresponds to a participant.\"\"\"\n",
    "\n",
    "    assert len(selected_participants) == self.L\n",
    "    answers = {}\n",
    "\n",
    "    for participant in selected_participants:\n",
    "\n",
    "      # ensure that the participant is an integer\n",
    "      participant = int(participant)\n",
    "\n",
    "      # whether or not the participant's answer is correct\n",
    "      correct_prob = self.p[participant]\n",
    "      correct = np.random.binomial(1, correct_prob)\n",
    "\n",
    "      # append the participant's answer to answers\n",
    "      if correct:\n",
    "        answers[participant] = self.correct_answer\n",
    "      else:\n",
    "        answers[participant] = self.wrong_answer\n",
    "\n",
    "    return answers\n",
    "\n",
    "  def get_correct_answer_and_reward(self, answer):\n",
    "    \"\"\"get correct answer and reward based on the agent's answer.\"\"\"\n",
    "\n",
    "    if answer == self.correct_answer:\n",
    "      return self.correct_answer, 1\n",
    "    else:\n",
    "      return self.correct_answer, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gREaypRF1flO"
   },
   "source": [
    "### Part c.2: the Thompson sampling agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "zwEBRv3Y0D_R"
   },
   "outputs": [],
   "source": [
    "#@title Thompson sampling agent for online crowdsourcing\n",
    "\n",
    "class TS4CrowdSourcing:\n",
    "\n",
    "  def __init__(self, env):\n",
    "\n",
    "    self.env = env\n",
    "\n",
    "    self.M = env.M\n",
    "    self.L = env.L\n",
    "\n",
    "    self.q = env.q\n",
    "\n",
    "  # call this function to run the experiments\n",
    "  # horizon is the time horizon\n",
    "  # num_sim is the number of simulations\n",
    "  def run(self, horizon, num_sim=1):\n",
    "\n",
    "    # placeholder for the results\n",
    "    reward_matrix = np.zeros((num_sim, horizon))\n",
    "\n",
    "    for s in range(num_sim):\n",
    "\n",
    "      # initialize the alpha matrix and the beta matrix for Thompson sampling\n",
    "      alpha_vec = np.ones(self.M)\n",
    "      beta_vec = np.ones(self.M)\n",
    "\n",
    "      for t in range(horizon):\n",
    "\n",
    "        # sample a vector of p's\n",
    "        p_vec = np.random.beta(alpha_vec, beta_vec)\n",
    "\n",
    "        self.env.get_question()\n",
    "\n",
    "        ############### Start of Code Modification ##########################\n",
    "        ### Write Code to select L participants; 2 to 4 lines of code expected\n",
    "        selected_participants = \n",
    "        ################### End of Code Modification ########################\n",
    "\n",
    "        participant_answers = self.env.get_answers(selected_participants)\n",
    "\n",
    "        ############### Start of Code Modification ##########################\n",
    "        ### Write Code to select the agent's answer based on participants' answers; \n",
    "        ### 8-15 lines of code expected\n",
    "        agent_answer = \n",
    "        ################### End of Code Modification ########################\n",
    "\n",
    "        correct_answer, reward = self.env.get_correct_answer_and_reward(agent_answer)\n",
    "\n",
    "        # update posterior\n",
    "        for i in range(self.L):\n",
    "          participant = selected_participants[i]\n",
    "\n",
    "          if participant_answers[participant] == correct_answer:\n",
    "            alpha_vec[participant] += 1\n",
    "          else:\n",
    "            beta_vec[participant] += 1\n",
    "\n",
    "        reward_matrix[s,t] = reward\n",
    "\n",
    "    # compute the mean per-step reward \n",
    "    reward_mean = np.mean(reward_matrix, axis=0)\n",
    "    return reward_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4bY8Bx31ssV"
   },
   "source": [
    "### Part c.3: Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcNmNjBk1ohn"
   },
   "outputs": [],
   "source": [
    "#@title Experiment setup\n",
    "\n",
    "M = 20\n",
    "L = 3\n",
    "\n",
    "q = 0.55\n",
    "\n",
    "horizon = 1000\n",
    "num_sim = 500\n",
    "\n",
    "env = CrowdsourcingEnv(M, L, q)\n",
    "agent = TS4CrowdSourcing(env)\n",
    "reward_mean = agent.run(horizon, num_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPQTPLmo2SQJ"
   },
   "outputs": [],
   "source": [
    "#@title Plot the results\n",
    "\n",
    "plt.plot(range(horizon), reward_mean, linewidth=2)\n",
    "\n",
    "plt.xlabel(\"$t$\", fontsize=14)\n",
    "plt.ylabel(\"per-step reward\", fontsize=14)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
