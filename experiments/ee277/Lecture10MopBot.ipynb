{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ck1w8Hp3xECr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# cleaning robot example\n",
    "class MopBot:\n",
    "  def __init__(self,\n",
    "               layout = np.array([[0, 0, 0, 0, 0]]),\n",
    "               prob_spill = 0.1):\n",
    "    # elements of the map are:\n",
    "    # 0: unobstructed cell\n",
    "    # 1: obstructed cell\n",
    "    # 5: unobstructed cell with spill (5 looks like S)\n",
    "    # 6: unobstructed cell with agent (think Baymax of Big Hero 6)\n",
    "    self.layout = layout\n",
    "    self.map = layout.copy()\n",
    "    self.map[0,0] = 6\n",
    "    self.prob_spill = prob_spill\n",
    "  \n",
    "  def step(self, action):\n",
    "    # determine current and target location\n",
    "    location = np.where(self.map == 6)\n",
    "    location = [int(location[0]),int(location[1])]\n",
    "    target = location.copy()\n",
    "    if action == 'left':\n",
    "      target[1] -= 1\n",
    "    elif action == 'right':\n",
    "      target[1] += 1\n",
    "    elif action == 'up':\n",
    "      target[0] -= 1\n",
    "    elif action == 'down':\n",
    "      target[0] += 1\n",
    "\n",
    "    # move agent if unobstructed\n",
    "    height,width = np.shape(self.map)\n",
    "    if (0 <= target[0] < height) and (0 <= target[1] < width) and (self.map[target[0],target[1]] != 1):\n",
    "      self.map[location[0],location[1]] = 0\n",
    "      self.map[target[0],target[1]] = 6\n",
    "\n",
    "    # new spill\n",
    "    spills = np.random.binomial(1, self.prob_spill, (height, width))\n",
    "    self.map += (self.map == 0) * spills * 5\n",
    "\n",
    "    observation = self.map\n",
    "    return observation\n",
    "\n",
    "  def map_to_state(self, map):\n",
    "    # state = concatenate([row, col], spill_configuration_array)\n",
    "    row,col = np.where(map == 6)\n",
    "    spills = (map == 5).flatten()\n",
    "    state = np.concatenate((row, col, spills))\n",
    "    return tuple(state)\n",
    "\n",
    "  def state_to_map(self, state):\n",
    "    location = state[:2]\n",
    "    spills = np.array(state[2:])\n",
    "    map = self.layout.copy()\n",
    "    map[location[0],location[1]] = 6\n",
    "    spill_map = spills.reshape(map.shape)\n",
    "    map += (map == 0) * spill_map * 5\n",
    "    return map\n",
    "\n",
    "  def state_value_table(self):\n",
    "    # allocate table with one value per state, initialized to zero\n",
    "    height,width = np.shape(self.map)\n",
    "    table = np.zeros([height,width] + height*width*[2])\n",
    "    return table\n",
    "\n",
    "  def state_list(self):\n",
    "    height,width = np.shape(self.map)\n",
    "    spill_list = [[]]\n",
    "    for idx in range(height*width):\n",
    "      spill_list = [s + [0] for s in spill_list] + [s + [1] for s in spill_list]\n",
    "    return [tuple(np.concatenate(([row], [col], spills)))\n",
    "            for row in range(height) \n",
    "            for col in range(width)\n",
    "            for spills in spill_list]\n",
    "\n",
    "  def transition_probs(self, state, action):\n",
    "    # compute state transition probabilities\n",
    "    # transition probs is an array with one probability per state\n",
    "    map = self.state_to_map(state)\n",
    "    location = np.where(map == 6)\n",
    "    location = [int(location[0]),int(location[1])]\n",
    "\n",
    "    # move agent\n",
    "    target = location.copy()\n",
    "    if action == 'left':\n",
    "      target[1] -= 1\n",
    "    elif action == 'right':\n",
    "      target[1] += 1\n",
    "    elif action == 'up':\n",
    "      target[0] -= 1\n",
    "    elif action == 'down':\n",
    "      target[0] += 1\n",
    "    # move agent if unobstructed\n",
    "    height,width = np.shape(map)\n",
    "    if (0 <= target[0] < height) and (0 <= target[1] < width) and (map[target[0],target[1]] != 1):\n",
    "      map[location[0],location[1]] = 0\n",
    "      map[target[0],target[1]] = 6\n",
    "\n",
    "    transition_probs = self.state_value_table()\n",
    "\n",
    "    # iterate through possible spill configurations\n",
    "    spill_list = [np.array([], dtype='int64')]\n",
    "    for idx in range(height*width):\n",
    "      spill_list = [np.concatenate((s,[0])) for s in spill_list] + [np.concatenate((s,[1])) for s in spill_list]\n",
    "    for spills in spill_list:\n",
    "      spill_map = spills.reshape(map.shape)\n",
    "      new_map = map.copy()\n",
    "      new_map += (new_map == 0) * spill_map * 5\n",
    "      new_state = self.map_to_state(new_map)\n",
    "      print(spills)\n",
    "      transition_probs[new_state] += np.power(self.prob_spill, np.sum(spills)) * np.power(1-self.prob_spill, np.sum(spills==0))      \n",
    "    return transition_probs\n",
    "\n",
    "\n",
    "def reward(map):\n",
    "  return -np.sum(map==5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "ZqLKRjYuXYHr",
    "outputId": "0789fa1d-f6d4-414c-ed46-134c791dccab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['right', 'right', 'stay', 'left', 'left']\n",
      "['right', 'left', 'left', 'left', 'left']\n"
     ]
    }
   ],
   "source": [
    "## code for planning via value iteration in the simple default case of a single hallway\n",
    "mb = MopBot()\n",
    "\n",
    "# compute optimal action value function\n",
    "action_list = ['left','right','stay'] # note that these actions are specialized to the default case of a single hallway\n",
    "Q = {a:mb.state_value_table() for a in action_list}\n",
    "for iter in range(100):\n",
    "  Q_update = {a:mb.state_value_table() for a in action_list}\n",
    "  for a in action_list:\n",
    "    for state in mb.state_list():\n",
    "      probs = mb.transition_probs(state, a)\n",
    "      Q_max = np.maximum.reduce([Q[b] for b in action_list])\n",
    "      Q_update[a][state] = reward(mb.state_to_map(state)) + np.sum(np.multiply(probs, Q_max))\n",
    "  Q = Q_update\n",
    "\n",
    "# print map of optimal actions at time 0, with no spills\n",
    "policy = [None, None, None, None, None] # note that the dimensions are specialized to the default case of a single hallway\n",
    "for location in range(len(policy)):\n",
    "  map = mb.layout.copy()\n",
    "  map[0,location] = 6\n",
    "  state = mb.map_to_state(map)\n",
    "  policy[location] = action_list[np.argmax([Q[a][state] for a in action_list])]\n",
    "print(policy)\n",
    "\n",
    "# print map of optimal actions at time 0, with one spill at the left wall\n",
    "policy = [None, None, None, None, None] # note that the dimensions are specialized to the default case of a single hallway\n",
    "for location in range(len(policy)):\n",
    "  map = mb.layout.copy()\n",
    "  map[0,0] = 5\n",
    "  map[0,location] = 6\n",
    "  state = mb.map_to_state(map)\n",
    "  policy[location] = action_list[np.argmax([Q[a][state] for a in action_list])]\n",
    "print(policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "IuiiJ7tnCpIo",
    "outputId": "53e4dfaf-95f0-447d-f0c9-6e43e54eacc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['right', 'right', 'stay', 'left', 'left']\n",
      "['right', 'left', 'left', 'left', 'left']\n"
     ]
    }
   ],
   "source": [
    "## code for planning via Q-learning in the simple default case of a single hallway\n",
    "stepsize = 0.01\n",
    "discount = 0.99\n",
    "\n",
    "mb = MopBot()\n",
    "action_list = ['left','right','stay'] # note that these actions are specialized to the default case of a single hallway\n",
    "Q = {a:mb.state_value_table() for a in action_list}\n",
    "state = mb.map_to_state(mb.map)\n",
    "for iter in range(10000000):\n",
    "  action = action_list[np.argmax([Q[a][state] for a in action_list])]\n",
    "  observation = mb.step(action)\n",
    "  next_state = mb.map_to_state(observation)\n",
    "  Q_max = np.maximum.reduce([Q[b] for b in action_list])\n",
    "  Q[action][state] += stepsize * (reward(observation) + discount*Q_max[next_state] - Q[action][state])\n",
    "  state = next_state\n",
    "\n",
    "# print map of optimal actions at time 0, with no spills\n",
    "policy = [None, None, None, None, None] # note that the dimensions are specialized to the default case of a single hallway\n",
    "for location in range(len(policy)):\n",
    "  map = mb.layout.copy()\n",
    "  map[0,location] = 6\n",
    "  state = mb.map_to_state(map)\n",
    "  policy[location] = action_list[np.argmax([Q[a][state] for a in action_list])]\n",
    "print(policy)\n",
    "\n",
    "# print map of optimal actions at time 0, with one spill at the left wall\n",
    "policy = [None, None, None, None, None] # note that the dimensions are specialized to the default case of a single hallway\n",
    "for location in range(len(policy)):\n",
    "  map = mb.layout.copy()\n",
    "  map[0,0] = 5\n",
    "  map[0,location] = 6\n",
    "  state = mb.map_to_state(map)\n",
    "  policy[location] = action_list[np.argmax([Q[a][state] for a in action_list])]\n",
    "print(policy)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
