{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4wMUjwX1xR8t",
        "fuMfYl6AN2Tc"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXF8ce0zoZKV"
      },
      "source": [
        "### Install bsuite along with the baseline agents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH01kaf3oZKf"
      },
      "source": [
        "! pip install -q bsuite[baselines_jax]  # quiet import along with agent implemented in jax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAq-0fy4oZKx"
      },
      "source": [
        "### Imports (click to expand the hidden cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig6gUsaYoZKy"
      },
      "source": [
        "import math\n",
        "import time\n",
        "import bsuite\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from typing import Any, Callable, NamedTuple, Sequence\n",
        "\n",
        "from bsuite.baselines import base\n",
        "from bsuite.baselines.utils import replay\n",
        "\n",
        "import dm_env\n",
        "from dm_env import specs\n",
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import rlax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrOeZGd5Nqj7"
      },
      "source": [
        "## Implementation of DQN and bootstrapped DQN agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLo4ODwPlN48"
      },
      "source": [
        "### Functions that need to be completed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWFJJaQHlRP8"
      },
      "source": [
        "def sample_td_error(q_t, a_t, r, discount, q_next):\n",
        "  \"\"\"Write a function for computing TD error for a single sample.\n",
        "\n",
        "  For a sample (s_t, a_t, r, s_{t+1}),\n",
        "  q_t: Q-values corresponding to state s_t. shape = (number of actions)\n",
        "  a_t: action. An integer. We assume that all the actions are indexed \n",
        "       from 0 to number of actions -1.\n",
        "  r: reward,\n",
        "  q_next: Q_values correspondng to the next state s_{t+1}.\n",
        "          shape = (number of actions)\n",
        "  \"\"\"\n",
        "  pass\n",
        "\n",
        "\n",
        "def epsilon_greedy(q_values, num_actions, epsilon):\n",
        "  \"\"\" Epsilon-greedy action selection scheme.\n",
        "\n",
        "  q_values: Q-values corresponding to different actions of the current state.\n",
        "            shape = (1, number of actions)\n",
        "  num_actions: number of actions. (same as the size of q_values array)\n",
        "  epsilon: The probability with which we select a random action.\n",
        "           a float value in between 0 and 1. \n",
        "  We assume that all the actions are indexed from 0 to number of actions -1.\n",
        "  \"\"\" \n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wMUjwX1xR8t"
      },
      "source": [
        "### DQN agent code\n",
        "\n",
        "**You don't need to modify this part of the code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGf1EqejxChr"
      },
      "source": [
        "class TrainingState(NamedTuple):\n",
        "  \"\"\"Holds the agent's training state.\"\"\"\n",
        "  params: hk.Params\n",
        "  target_params: hk.Params\n",
        "  opt_state: Any\n",
        "  step: int\n",
        "\n",
        "class DQN(base.Agent):\n",
        "  \"\"\"A simple DQN agent using JAX.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      obs_spec: specs.Array,\n",
        "      action_spec: specs.DiscreteArray,\n",
        "      network: Callable[[jnp.ndarray], jnp.ndarray],\n",
        "      optimizer: optax.GradientTransformation,\n",
        "      batch_size: int,\n",
        "      epsilon: float,\n",
        "      rng: hk.PRNGSequence,\n",
        "      discount: float,\n",
        "      replay_capacity: int,\n",
        "      min_replay_size: int,\n",
        "      sgd_period: int,\n",
        "      target_update_period: int,\n",
        "  ):\n",
        "    self.name = 'dqn'\n",
        "\n",
        "    # Transform the (impure) network into a pure function.\n",
        "    network = hk.without_apply_rng(hk.transform(network, apply_rng=True))\n",
        "\n",
        "    # Define loss function.\n",
        "    def loss(params: hk.Params,\n",
        "             target_params: hk.Params,\n",
        "             transitions: Sequence[jnp.ndarray]) -> jnp.ndarray:\n",
        "      \"\"\"Computes the standard TD(0) Q-learning loss on batch of transitions.\"\"\"\n",
        "      o_tm1, a_tm1, r_t, d_t, o_t = transitions\n",
        "      q_tm1 = network.apply(params, o_tm1)\n",
        "      q_t = network.apply(target_params, o_t)\n",
        "      q_t = jax.lax.stop_gradient(q_t)\n",
        "      batch_q_learning = jax.vmap(sample_td_error)\n",
        "      td_error = batch_q_learning(q_tm1, a_tm1, r_t, discount * d_t, q_t)\n",
        "      return jnp.mean(td_error**2)\n",
        "\n",
        "    # Define update function.\n",
        "    @jax.jit\n",
        "    def sgd_step(state: TrainingState,\n",
        "                 transitions: Sequence[jnp.ndarray]) -> TrainingState:\n",
        "      \"\"\"Performs an SGD step on a batch of transitions.\"\"\"\n",
        "      gradients = jax.grad(loss)(state.params, state.target_params, transitions)\n",
        "      updates, new_opt_state = optimizer.update(gradients, state.opt_state)\n",
        "      new_params = optax.apply_updates(state.params, updates)\n",
        "\n",
        "      return TrainingState(\n",
        "          params=new_params,\n",
        "          target_params=state.target_params,\n",
        "          opt_state=new_opt_state,\n",
        "          step=state.step + 1)\n",
        "\n",
        "    # Initialize the networks and optimizer.\n",
        "    dummy_observation = np.zeros((1, *obs_spec.shape), jnp.float32)\n",
        "    initial_params = network.init(next(rng), dummy_observation)\n",
        "    initial_target_params = network.init(next(rng), dummy_observation)\n",
        "    initial_opt_state = optimizer.init(initial_params)\n",
        "\n",
        "    # This carries the agent state relevant to training.\n",
        "    self._state = TrainingState(\n",
        "        params=initial_params,\n",
        "        target_params=initial_target_params,\n",
        "        opt_state=initial_opt_state,\n",
        "        step=0)\n",
        "    self._sgd_step = sgd_step\n",
        "    self._forward = jax.jit(network.apply)\n",
        "    self._replay = replay.Replay(capacity=replay_capacity)\n",
        "\n",
        "    # Store hyperparameters.\n",
        "    self._num_actions = action_spec.num_values\n",
        "    self._batch_size = batch_size\n",
        "    self._sgd_period = sgd_period\n",
        "    self._target_update_period = target_update_period\n",
        "    self._epsilon = epsilon\n",
        "    self._total_steps = 0\n",
        "    self._min_replay_size = min_replay_size\n",
        "\n",
        "  def select_action(self, timestep: dm_env.TimeStep) -> base.Action:\n",
        "    \"\"\"Selects actions according to an epsilon-greedy policy.\"\"\"\n",
        "    observation = timestep.observation[None, ...]\n",
        "    q_values = self._forward(self._state.params, observation)\n",
        "    action = epsilon_greedy(q_values, self._num_actions, self._epsilon)\n",
        "    return int(action)\n",
        "\n",
        "  def update(\n",
        "      self,\n",
        "      timestep: dm_env.TimeStep,\n",
        "      action: base.Action,\n",
        "      new_timestep: dm_env.TimeStep,\n",
        "  ):\n",
        "    \"\"\"Adds transition to replay and periodically does SGD.\"\"\"\n",
        "    # Add this transition to replay.\n",
        "    self._replay.add([\n",
        "        timestep.observation,\n",
        "        action,\n",
        "        new_timestep.reward,\n",
        "        new_timestep.discount,\n",
        "        new_timestep.observation,\n",
        "    ])\n",
        "\n",
        "    self._total_steps += 1\n",
        "    if self._total_steps % self._sgd_period != 0:\n",
        "      return\n",
        "\n",
        "    if self._replay.size < self._min_replay_size:\n",
        "      return\n",
        "\n",
        "    # Do a batch of SGD.\n",
        "    transitions = self._replay.sample(self._batch_size)\n",
        "    self._state = self._sgd_step(self._state, transitions)\n",
        "\n",
        "    # Periodically update target parameters.\n",
        "    if self._state.step % self._target_update_period == 0:\n",
        "      self._state = self._state._replace(target_params=self._state.params)\n",
        "\n",
        "\n",
        "def dqn_agent(obs_spec: specs.Array,\n",
        "                  action_spec: specs.DiscreteArray,\n",
        "                  seed: int = 0) -> base.Agent:\n",
        "  \"\"\"Initialize a DQN agent with default parameters.\"\"\"\n",
        "\n",
        "  np.random.seed(seed)\n",
        "\n",
        "  def network(inputs: jnp.ndarray) -> jnp.ndarray:\n",
        "    flat_inputs = hk.Flatten()(inputs)\n",
        "    mlp = hk.nets.MLP([50, 50, action_spec.num_values])\n",
        "    action_values = mlp(flat_inputs)\n",
        "    return action_values\n",
        "\n",
        "  return DQN(\n",
        "      obs_spec=obs_spec,\n",
        "      action_spec=action_spec,\n",
        "      network=network,\n",
        "      optimizer=optax.adam(1e-3),\n",
        "      batch_size=128,\n",
        "      discount=0.99,\n",
        "      replay_capacity=10000,\n",
        "      min_replay_size=128,\n",
        "      sgd_period=1,\n",
        "      target_update_period=4,\n",
        "      epsilon=0.2,\n",
        "      rng=hk.PRNGSequence(seed),\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuMfYl6AN2Tc"
      },
      "source": [
        "### Boot DQN agent code\n",
        "\n",
        "**You don't need to modify this part of the code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y7TILD9N44N"
      },
      "source": [
        "class TrainingState(NamedTuple):\n",
        "  params: hk.Params\n",
        "  target_params: hk.Params\n",
        "  opt_state: Any\n",
        "  step: int\n",
        "\n",
        "\n",
        "class BootstrappedDqn(base.Agent):\n",
        "  \"\"\"Bootstrapped DQN with randomized prior functions.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      obs_spec: specs.Array,\n",
        "      action_spec: specs.DiscreteArray,\n",
        "      network: Callable[[jnp.ndarray], jnp.ndarray],\n",
        "      num_ensemble: int,\n",
        "      batch_size: int,\n",
        "      discount: float,\n",
        "      replay_capacity: int,\n",
        "      min_replay_size: int,\n",
        "      sgd_period: int,\n",
        "      target_update_period: int,\n",
        "      optimizer: optax.GradientTransformation,\n",
        "      mask_prob: float,\n",
        "      noise_scale: float,\n",
        "      epsilon_fn: Callable[[int], float] = lambda _: 0.,\n",
        "      seed: int = 1,\n",
        "  ):\n",
        "    self.name = 'boot dqn'\n",
        "    # Transform the (impure) network into a pure function.\n",
        "    network = hk.without_apply_rng(hk.transform(network, apply_rng=True))\n",
        "\n",
        "    # Define loss function, including bootstrap mask `m_t` & reward noise `z_t`.\n",
        "    def loss(params: hk.Params, target_params: hk.Params,\n",
        "             transitions: Sequence[jnp.ndarray]) -> jnp.ndarray:\n",
        "      \"\"\"Q-learning loss with added reward noise + half-in bootstrap.\"\"\"\n",
        "      o_tm1, a_tm1, r_t, d_t, o_t, m_t, z_t = transitions\n",
        "      q_tm1 = network.apply(params, o_tm1)\n",
        "      q_t = network.apply(target_params, o_t)\n",
        "      r_t += noise_scale * z_t\n",
        "      batch_q_learning = jax.vmap(sample_td_error)\n",
        "      td_error = batch_q_learning(q_tm1, a_tm1, r_t, discount * d_t, q_t)\n",
        "      return jnp.mean(m_t * td_error**2)\n",
        "\n",
        "    # Define update function for each member of ensemble..\n",
        "    @jax.jit\n",
        "    def sgd_step(state: TrainingState,\n",
        "                 transitions: Sequence[jnp.ndarray]) -> TrainingState:\n",
        "      \"\"\"Does a step of SGD for the whole ensemble over `transitions`.\"\"\"\n",
        "\n",
        "      gradients = jax.grad(loss)(state.params, state.target_params, transitions)\n",
        "      updates, new_opt_state = optimizer.update(gradients, state.opt_state)\n",
        "      new_params = optax.apply_updates(state.params, updates)\n",
        "\n",
        "      return TrainingState(\n",
        "          params=new_params,\n",
        "          target_params=state.target_params,\n",
        "          opt_state=new_opt_state,\n",
        "          step=state.step + 1)\n",
        "\n",
        "    # Initialize parameters and optimizer state for an ensemble of Q-networks.\n",
        "    rng = hk.PRNGSequence(seed)\n",
        "    dummy_obs = np.zeros((1, *obs_spec.shape), jnp.float32)\n",
        "    initial_params = [\n",
        "        network.init(next(rng), dummy_obs) for _ in range(num_ensemble)\n",
        "    ]\n",
        "    initial_target_params = [\n",
        "        network.init(next(rng), dummy_obs) for _ in range(num_ensemble)\n",
        "    ]\n",
        "    initial_opt_state = [optimizer.init(p) for p in initial_params]\n",
        "\n",
        "    # Internalize state.\n",
        "    self._ensemble = [\n",
        "        TrainingState(p, tp, o, step=0) for p, tp, o in zip(\n",
        "            initial_params, initial_target_params, initial_opt_state)\n",
        "    ]\n",
        "    self._forward = jax.jit(network.apply)\n",
        "    self._sgd_step = sgd_step\n",
        "    self._num_ensemble = num_ensemble\n",
        "    self._optimizer = optimizer\n",
        "    self._replay = replay.Replay(capacity=replay_capacity)\n",
        "\n",
        "    # Agent hyperparameters.\n",
        "    self._num_actions = action_spec.num_values\n",
        "    self._batch_size = batch_size\n",
        "    self._sgd_period = sgd_period\n",
        "    self._target_update_period = target_update_period\n",
        "    self._min_replay_size = min_replay_size\n",
        "    self._epsilon_fn = epsilon_fn\n",
        "    self._mask_prob = mask_prob\n",
        "\n",
        "    # Agent state.\n",
        "    self._active_head = self._ensemble[0]\n",
        "    self._total_steps = 0\n",
        "\n",
        "  def select_action(self, timestep: dm_env.TimeStep) -> base.Action:\n",
        "    \"\"\"Select values via Thompson sampling, then use epsilon-greedy policy.\"\"\"\n",
        "    self._total_steps += 1\n",
        "    batched_obs = timestep.observation[None, ...]\n",
        "    q_values = self._forward(self._active_head.params, batched_obs)\n",
        "    action = epsilon_greedy(q_values, self._num_actions,\n",
        "                            self._epsilon_fn(self._total_steps))\n",
        "    return int(action)\n",
        "\n",
        "  def update(\n",
        "      self,\n",
        "      timestep: dm_env.TimeStep,\n",
        "      action: base.Action,\n",
        "      new_timestep: dm_env.TimeStep,\n",
        "  ):\n",
        "    \"\"\"Update the agent: add transition to replay and periodically do SGD.\"\"\"\n",
        "\n",
        "    # Thompson sampling: every episode pick a new Q-network as the policy.\n",
        "    if new_timestep.last():\n",
        "      k = np.random.randint(self._num_ensemble)\n",
        "      self._active_head = self._ensemble[k]\n",
        "\n",
        "    # Generate bootstrapping mask & reward noise.\n",
        "    mask = np.random.binomial(1, self._mask_prob, self._num_ensemble)\n",
        "    noise = np.random.randn(self._num_ensemble)\n",
        "\n",
        "    # Make transition and add to replay.\n",
        "    transition = [\n",
        "        timestep.observation,\n",
        "        action,\n",
        "        np.float32(new_timestep.reward),\n",
        "        np.float32(new_timestep.discount),\n",
        "        new_timestep.observation,\n",
        "        mask,\n",
        "        noise,\n",
        "    ]\n",
        "    self._replay.add(transition)\n",
        "\n",
        "    if self._replay.size < self._min_replay_size:\n",
        "      return\n",
        "\n",
        "    # Periodically sample from replay and do SGD for the whole ensemble.\n",
        "    if self._total_steps % self._sgd_period == 0:\n",
        "      transitions = self._replay.sample(self._batch_size)\n",
        "      o_tm1, a_tm1, r_t, d_t, o_t, m_t, z_t = transitions\n",
        "      for k, state in enumerate(self._ensemble):\n",
        "        transitions = [o_tm1, a_tm1, r_t, d_t, o_t, m_t[:, k], z_t[:, k]]\n",
        "        self._ensemble[k] = self._sgd_step(state, transitions)\n",
        "\n",
        "    # Periodically update target parameters.\n",
        "    for k, state in enumerate(self._ensemble):\n",
        "      if state.step % self._target_update_period == 0:\n",
        "        self._ensemble[k] = state._replace(target_params=state.params)\n",
        "\n",
        "\n",
        "def boot_dqn_agent(\n",
        "    obs_spec: specs.Array,\n",
        "    action_spec: specs.DiscreteArray,\n",
        "    seed: int = 0,\n",
        "    num_ensemble: int = 10,\n",
        ") -> BootstrappedDqn:\n",
        "  \"\"\"Initialize a Bootstrapped DQN agent with default parameters.\"\"\"\n",
        "\n",
        "  np.random.seed(seed)\n",
        "\n",
        "  # Define network.\n",
        "  prior_scale = 5.\n",
        "  hidden_sizes = [50, 50]\n",
        "\n",
        "  def network(inputs: jnp.ndarray) -> jnp.ndarray:\n",
        "    \"\"\"Simple Q-network with randomized prior function.\"\"\"\n",
        "    net = hk.nets.MLP([*hidden_sizes, action_spec.num_values])\n",
        "    prior_net = hk.nets.MLP([*hidden_sizes, action_spec.num_values])\n",
        "    x = hk.Flatten()(inputs)\n",
        "    return net(x) + prior_scale * jax.lax.stop_gradient(prior_net(x))\n",
        "\n",
        "  optimizer = optax.adam(learning_rate=1e-3)\n",
        "  return BootstrappedDqn(\n",
        "      obs_spec=obs_spec,\n",
        "      action_spec=action_spec,\n",
        "      network=network,\n",
        "      batch_size=128,\n",
        "      discount=.99,\n",
        "      num_ensemble=num_ensemble,\n",
        "      replay_capacity=10000,\n",
        "      min_replay_size=128,\n",
        "      sgd_period=1,\n",
        "      target_update_period=4,\n",
        "      optimizer=optimizer,\n",
        "      mask_prob=1.,\n",
        "      noise_scale=0.,\n",
        "      epsilon_fn=lambda _: 0.,\n",
        "      seed=seed,\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1frschGOVz_"
      },
      "source": [
        "### Run DQN and Boot DQN agent on the deep sea environment of depth 10\n",
        "\n",
        "**You don't need to modify this part of the code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PXGhFEBOV0B"
      },
      "source": [
        "# @title run the agent and collect data\n",
        "num_simulations = 5\n",
        "num_episodes = 250\n",
        "\n",
        "deep_sea_sizes = [10]\n",
        "AGENT_LIST = [dqn_agent, boot_dqn_agent]\n",
        "\n",
        "results = []\n",
        "for seed in range(num_simulations):\n",
        "  for deep_sea_size in deep_sea_sizes:\n",
        "    env = bsuite.load('deep_sea', {'size': deep_sea_size, \n",
        "                                   'seed': seed, 'mapping_seed': seed})\n",
        "    for AGENT in AGENT_LIST:\n",
        "\n",
        "      agent = AGENT(obs_spec=env.observation_spec(),\n",
        "                    action_spec=env.action_spec(),\n",
        "                    seed=seed)\n",
        "      for episode in range(num_episodes):\n",
        "        # Run an episode.\n",
        "        timestep = env.reset()\n",
        "        reward = 0\n",
        "        while not timestep.last():\n",
        "          # Generate an action from the agent's policy.\n",
        "          action = agent.select_action(timestep)\n",
        "          # Step the environment.\n",
        "          new_timestep = env.step(action)\n",
        "          # Tell the agent about what just happened.\n",
        "          agent.update(timestep, action, new_timestep)\n",
        "          # Book-keeping.\n",
        "          timestep = new_timestep\n",
        "          reward += timestep.reward\n",
        "\n",
        "        result = {'episode': episode, 'reward': round(reward, 3),\n",
        "                  'regret': round(0.99 - reward, 3), 'seed': seed,\n",
        "                  'agent': agent.name, 'deep_sea_size': deep_sea_size}\n",
        "        results.append(result)\n",
        "        if episode % int(num_episodes/5) == 0:\n",
        "          print(result)\n",
        "df = pd.DataFrame(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOdpvgB9OV0L"
      },
      "source": [
        "### Plot results\n",
        "\n",
        "**You don't need to modify this part of the code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU-nIoROOV0M"
      },
      "source": [
        "ave_df = df.groupby(['episode', 'agent', 'deep_sea_size'])['reward', 'regret'].mean().reset_index()\n",
        "\n",
        "for deep_sea_size in ave_df.deep_sea_size.unique():\n",
        "  plt.figure()\n",
        "  for agent in ave_df.agent.unique():\n",
        "    agent_df = ave_df[(ave_df.agent==agent) & (ave_df.deep_sea_size==deep_sea_size)]\n",
        "    plt.plot(agent_df.episode, np.cumsum(agent_df.regret), label=agent)\n",
        "  plt.xlabel(r'episode', fontsize=20)\n",
        "  plt.ylabel('cumulative regret', fontsize=20)\n",
        "  plt.title('Deep sea of size '+str(deep_sea_size))\n",
        "  plt.legend(loc='best')\n",
        "  plt.ylim([0, 200])\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}