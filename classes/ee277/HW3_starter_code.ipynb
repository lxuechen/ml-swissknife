{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgI1_N6FIDNY"
   },
   "source": [
    "# Question 2: conditional variance as an approximation of mutual information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ShSL4hgEFqu"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PRKfWodjIcOY"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DKhBXBacIkzQ",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.038366256079300776"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The code here is just for you to see how to call the entropy function.\n",
    "a = 1\n",
    "b = 10\n",
    "conditional_entropy = beta.mean(a, b) * beta.entropy(a + 1, b) + (1 - beta.mean(a, b)) * beta.entropy(a, b + 1)\n",
    "mutual_information = beta.entropy(a, b) - conditional_entropy\n",
    "mutual_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def comp_mi(al, be):\n",
    "    conditional_entropy = beta.mean(al, be) * beta.entropy(al + 1, be) + (1 - beta.mean(al, be)) * beta.entropy(al, be + 1)\n",
    "    return beta.entropy(al, be) - conditional_entropy\n",
    "\n",
    "def comp_var(al, be):\n",
    "    # closed-form solution computed using law of total expectation\n",
    "    return 1 / (al + be + 1) ** 2. * (al * be / (al + be) ** 2.)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "alphs = np.linspace(start=0.1, stop=20)\n",
    "\n",
    "b1 = [comp_mi(alph, 1) for alph in alphs]\n",
    "b5 = [comp_mi(alph, 5) for alph in alphs]\n",
    "b20 = [comp_mi(alph, 20) for alph in alphs]\n",
    "plt.plot(alphs, b1, label=\"MI $b=1$\")\n",
    "plt.plot(alphs, b5, label=\"MI $b=5$\")\n",
    "plt.plot(alphs, b20, label=\"MI $b=20$\")\n",
    "\n",
    "b1 = [comp_var(alph, 1) for alph in alphs]\n",
    "b5 = [comp_var(alph, 5) for alph in alphs]\n",
    "b20 = [comp_var(alph, 20) for alph in alphs]\n",
    "plt.plot(alphs, b1, label=\"variance $b=1$\", linestyle='-.')\n",
    "plt.plot(alphs, b5, label=\"variance $b=5$\", linestyle='-.')\n",
    "plt.plot(alphs, b20, label=\"variance $b=20$\", linestyle='-.')\n",
    "\n",
    "plt.xlabel(\"$a$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcvcb9lFJNKg"
   },
   "source": [
    "# Question 3: Expected cumulative regret of Thompson sampling in the meeting room environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZvmrI8hdE8m"
   },
   "source": [
    "You should not implement the Thompson sampling agent here. Compute the regret analytically. Optionally you can plot the analytically computed cumulative regret here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ho_DdegKoQRl"
   },
   "source": [
    "# Question 4: Multi-item recommendation problem with per item cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruSOPgrHy7Do"
   },
   "source": [
    "## Install and load the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "_TqlnZ15ps8u"
   },
   "outputs": [],
   "source": [
    "!pip install -q optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "8xlRiTrzSKSh"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. You may be able work around this issue by building jaxlib from source.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [21]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01moptax\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mjnp\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch/lib/python3.10/site-packages/optax/__init__.py:17\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2019 DeepMind Technologies Limited. All Rights Reserved.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# ==============================================================================\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Optax: composable gradient processing and optimization, in JAX.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01moptax\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m experimental\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01moptax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_src\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01malias\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m adabelief\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01moptax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_src\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01malias\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m adafactor\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch/lib/python3.10/site-packages/optax/experimental/__init__.py:20\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2021 DeepMind Technologies Limited. All Rights Reserved.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# ==============================================================================\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Experimental features in Optax.\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \n\u001B[1;32m     17\u001B[0m \u001B[38;5;124;03mFeatures may be removed or modified at any time.\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01moptax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_src\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcomplex_valued\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m split_real_and_imaginary\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01moptax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_src\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcomplex_valued\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SplitRealAndImaginaryState\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch/lib/python3.10/site-packages/optax/_src/experimental/complex_valued.py:32\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Complex-valued optimization.\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \n\u001B[1;32m     17\u001B[0m \u001B[38;5;124;03mWhen using `split_real_and_imaginary` to wrap an optimizer, we split the complex\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;124;03mSee details at https://github.com/deepmind/optax/issues/196\u001B[39;00m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m NamedTuple, Union\n\u001B[0;32m---> 32\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mchex\u001B[39;00m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mjnp\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch/lib/python3.10/site-packages/chex/__init__.py:17\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2020 DeepMind Technologies Limited. All Rights Reserved.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# ==============================================================================\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Chex: Testing made fun, in JAX!\"\"\"\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mchex\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_src\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01masserts\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m assert_axis_dimension\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mchex\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_src\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01masserts\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m assert_axis_dimension_comparator\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mchex\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_src\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01masserts\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m assert_axis_dimension_gt\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch/lib/python3.10/site-packages/chex/_src/asserts.py:26\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01munittest\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01munittest\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mock\n\u001B[0;32m---> 26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mchex\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_src\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m asserts_internal \u001B[38;5;28;01mas\u001B[39;00m _ai\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mchex\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_src\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pytypes\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch/lib/python3.10/site-packages/chex/_src/asserts_internal.py:34\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Any, Sequence, Union, Callable, Optional, Set, Tuple, Type\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mabsl\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m logging\n\u001B[0;32m---> 34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mchex\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_src\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pytypes\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m checkify\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch/lib/python3.10/site-packages/chex/_src/pytypes.py:18\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Pytypes for arrays and scalars.\"\"\"\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Any, Iterable, Mapping, Union\n\u001B[0;32m---> 18\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mjnp\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch/lib/python3.10/site-packages/jax/__init__.py:35\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m _cloud_tpu_init\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# Confusingly there are two things named \"config\": the module and the class.\u001B[39;00m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# We want the exported object to be the class, so we first import the module\u001B[39;00m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# to make sure a later import doesn't overwrite the class.\u001B[39;00m\n\u001B[0;32m---> 35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config \u001B[38;5;28;01mas\u001B[39;00m _config_module\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m _config_module\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_src\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbasearray\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Array \u001B[38;5;28;01mas\u001B[39;00m Array\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch/lib/python3.10/site-packages/jax/config.py:17\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2018 The JAX Authors.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     14\u001B[0m \n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# TODO(phawkins): fix users of this alias and delete this file.\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_src\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch/lib/python3.10/site-packages/jax/_src/config.py:29\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mabsl\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m logging\n\u001B[0;32m---> 29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_src\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m lib\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_src\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m jax_jit\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_src\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m transfer_guard_lib\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch/lib/python3.10/site-packages/jax/_src/lib/__init__.py:98\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;66;03m# Before importing any C compiled modules from jaxlib, first import the CPU\u001B[39;00m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;66;03m# feature guard module to verify that jaxlib was compiled in a way that only\u001B[39;00m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;66;03m# uses instructions that are present on this machine.\u001B[39;00m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjaxlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcpu_feature_guard\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mcpu_feature_guard\u001B[39;00m\n\u001B[0;32m---> 98\u001B[0m \u001B[43mcpu_feature_guard\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck_cpu_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjaxlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mxla_client\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mxla_client\u001B[39;00m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjaxlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlapack\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mlapack\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. You may be able work around this issue by building jaxlib from source."
     ]
    }
   ],
   "source": [
    "import optax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotnine as gg\n",
    "import random\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from typing import Sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sympy.utilities.iterables import multiset_permutations\n",
    "\n",
    "colors = {0: 'b',\n",
    "          1: 'g',\n",
    "          2: 'r',\n",
    "          3: 'c',\n",
    "          4: 'm',\n",
    "          5: 'y'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohjfF0qGy2qu"
   },
   "source": [
    "## Multi-item recommendation Environment\n",
    "\n",
    "A multi-item reccomendation environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BgaLWqQotdSK"
   },
   "outputs": [],
   "source": [
    "class MultiItemRecommendationEnv(object):\n",
    "  \"\"\"Environment representing a multi-item recommendation environment.\"\"\"\n",
    "\n",
    "  def __init__(self, num_item, num_slot, sigma_p=1.0):\n",
    "    self._num_item = num_item  # Total number of items that can be recommended\n",
    "    self._num_slot = num_slot  # Number of available sltos\n",
    "    self._sigma_p = sigma_p\n",
    "    self._rng = np.random.default_rng()\n",
    "    self.reset()\n",
    "\n",
    "  def reset(self, seed: int=None):\n",
    "    self._rng = np.random.default_rng(seed)\n",
    "    # reset the environment - will generate a new environment\n",
    "    self._theta = np.minimum(\n",
    "        self._rng.normal(size=self._num_item) * self._sigma_p - 1.0 * self._sigma_p,\n",
    "        0.0)\n",
    "\n",
    "  def _validate_action(self, action):\n",
    "    action = np.array(action)\n",
    "    assert len(action) == self._num_item, f'action has wrong dimension {len(action)}.'\n",
    "    return action\n",
    "\n",
    "  def step(self, action: Sequence[int]):\n",
    "    # select items to display based on the action\n",
    "    action = self._validate_action(action)\n",
    "    probs = self.compute_selection_probs(action)\n",
    "    return self._rng.choice(self._num_item + 1, p=probs)\n",
    "\n",
    "  def compute_selection_probs(self, action):\n",
    "    action = self._validate_action(action)\n",
    "    logits = self._theta * action\n",
    "    logits[action <= 0.0] = -np.inf  # Set the logit for non selected item to -Inf\n",
    "    logits = np.insert(logits, 0, 0.0)  # Add logits for the no-item-selected observation\n",
    "    probs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "    return probs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EeI-q589zAhb"
   },
   "source": [
    "## Agent for multi-item recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wruhIqzbqD1_"
   },
   "source": [
    "### Optimization procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V7TN2WLs9wcV"
   },
   "outputs": [],
   "source": [
    "\n",
    "def one_hot(indices, num_items: int):\n",
    "  \"\"\"Returns a one-hot encoding with shape [num_items+1, arr.size].\n",
    "  \"\"\"\n",
    "  indices = np.array(indices)\n",
    "  assert np.max(indices) <= num_items, \"Input indices overflow.\"\n",
    "  return np.eye(num_items+1)[indices].T\n",
    "\n",
    "\n",
    "class MLR(object):\n",
    "  \"\"\"Multinomial Logistic Regression.\n",
    "  \n",
    "  Internal variables:\n",
    "  _num_items: the number of items \n",
    "  _stepsize: the fixed constant stepsize in Newton's method\n",
    "  _num_samples: the number of data samples.\n",
    "  _action_history: np.ndarray with shape [_num_items, _num_samples], a binary\n",
    "      matrix containing the set of items presented to the user in each data \n",
    "      sample.  The first row of _action_history is always all-zero.\n",
    "  _observation_history: np.ndarray with shape [_num_items, _num_samples], each \n",
    "      column is a one-hot vector representing the item chosen by the user.  The\n",
    "      zeroth item is the empty item.\n",
    "  _sigma_p_squared: the regularization hyperparameter.\n",
    "  _theta: the current parameters.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      num_items: int, # total number of items to display to customer \n",
    "      sigma_p_squared: float = 1.0,\n",
    "      stepsize: float = 0.75,\n",
    "  ):\n",
    "    self._num_items = num_items \n",
    "    self._stepsize = stepsize\n",
    "    self._num_samples = 0\n",
    "    self._action_history = np.zeros((self._num_items+1, 0))\n",
    "    self._observation_history = np.zeros((self._num_items+1, 0)) \n",
    "    self._sigma_p_squared = sigma_p_squared\n",
    "    self._theta = np.zeros((self._num_items,))\n",
    "\n",
    "    # An all-zero vector is added to the first column of item_embeddings to \n",
    "    # represent the zero (empty) item.\n",
    "    self._item_embeddings = np.concatenate(\n",
    "        (np.zeros((num_items, 1)), np.eye(num_items)), axis=1)\n",
    "\n",
    "  @property \n",
    "  def coef_(self):\n",
    "    return self._theta\n",
    "  \n",
    "  @property \n",
    "  def stepsize(self):\n",
    "    return self._stepsize\n",
    "  \n",
    "  @stepsize.setter\n",
    "  def stepsize(self, new_stepsize: float):\n",
    "    self._stepsize = new_stepsize \n",
    "\n",
    "  def value_gradient_hessian(self):\n",
    "\n",
    "    # probabilities\n",
    "    exp_item_logits = np.exp(np.dot(self._item_embeddings.T, self._theta))\n",
    "    prob = np.expand_dims(exp_item_logits, axis=1) * self._action_history\n",
    "    prob /= np.sum(prob, axis=0, keepdims=True)\n",
    "\n",
    "    # value\n",
    "    value = 0.5 / self._sigma_p_squared * np.sum(np.square(self._theta))\n",
    "    item_logits = np.dot(self._item_embeddings.T, self._theta)\n",
    "    value -= np.sum(np.dot(self._observation_history.T, item_logits))\n",
    "    value += np.sum(np.log(np.dot(exp_item_logits,\n",
    "                                  self._action_history)))\n",
    "\n",
    "    # gradient of log-likelihood\n",
    "    grad = self._theta / self._sigma_p_squared\n",
    "    grad -= np.sum(np.dot(self._item_embeddings, self._observation_history), axis=-1)\n",
    "    grad += np.sum(np.dot(self._item_embeddings, prob), axis=-1)\n",
    "    \n",
    "    # hessian of log-likelihood\n",
    "    hess = np.eye(self._num_items) / self._sigma_p_squared\n",
    "    prob_feat = np.dot(self._item_embeddings, prob)\n",
    "    hess -= np.dot(prob_feat, prob_feat.T) \n",
    "    feat_prob_feat = np.expand_dims(self._item_embeddings, axis=-1\n",
    "                                    ) * np.expand_dims(prob, axis=0)\n",
    "    feat_prob_feat = np.tensordot(feat_prob_feat, self._item_embeddings, [[1], [1]])\n",
    "    hess += np.sum(feat_prob_feat, axis=1)\n",
    "    return value, grad, hess\n",
    "\n",
    "  def fit(self, actions=None, observations=None, num_newton_steps=10, verbose=False):\n",
    "    if actions is not None:\n",
    "      num_new_samples = actions.shape[1]\n",
    "      actions = np.r_[np.ones((1, num_new_samples)), actions]\n",
    "      self._action_history = actions\n",
    "    if observations is not None:\n",
    "      self._observation_history = one_hot(observations, self._num_items)\n",
    "    for _ in range(num_newton_steps):\n",
    "      loss, grad, hess = self.value_gradient_hessian()\n",
    "      self._theta -= self._stepsize * np.dot(np.linalg.inv(hess), grad)\n",
    "      if verbose:\n",
    "        print('loss:', loss, 'grad:', grad, 'hessian:', hess)\n",
    "    return self._theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oidcgSzSqKDY"
   },
   "source": [
    "### Helpful functions (**You need to change the code over here**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ArpBdWtX8Uwl"
   },
   "outputs": [],
   "source": [
    "# Constructing an agent which can learn only from the reward or from all the observations\n",
    "\n",
    "def choose_optimal_assortment_for_num_slots(theta, num_slot, rng=None):\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng()\n",
    "  num_item = len(theta)\n",
    "  theta_sorted_idx = np.argsort(-theta)\n",
    "  theta_thr = theta[theta_sorted_idx[num_slot - 1]]\n",
    "  theta_thr_idx = np.nonzero(theta == theta_thr)[0]\n",
    "  theta_opt_idx = np.setdiff1d(theta_sorted_idx[:num_slot], theta_thr_idx, True)\n",
    "  theta_opt_idx = np.concatenate((\n",
    "      theta_opt_idx,\n",
    "      rng.choice(theta_thr_idx, size=num_slot - len(theta_opt_idx), replace=False)))\n",
    "  selected_items = np.zeros(num_item)\n",
    "  selected_items[theta_opt_idx] = 1\n",
    "  return selected_items\n",
    "\n",
    "def select_opt_num_slots(theta, max_num_slot, cost_per_slot):\n",
    "  ############### Start of Code Modification ################################\n",
    "  ### Please implement to choose the optimal number of slots here\n",
    "  ###########################################################################\n",
    "\n",
    "  optimal_num_slots = ...\n",
    "  return optimal_num_slots\n",
    "  \n",
    "  ################### End of Code Modification ##############################\n",
    "\n",
    "def choose_optimal_assortment(theta, max_num_slot, rng=None, cost_per_slot:float=0):\n",
    "  opt_num_slots = select_opt_num_slots(theta, max_num_slot, cost_per_slot)\n",
    "  return choose_optimal_assortment_for_num_slots(theta, opt_num_slots, rng)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VeJleSFqTSG"
   },
   "source": [
    "### Laplace agent which learns from observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YgHfRS3oqWkT"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function computes the inverse Hessian matrix of the loss function\n",
    "theta.shape = (dim)\n",
    "X.shape = (num_samples, dim)\n",
    "'''\n",
    "def get_hessian(theta, X, sigma_p_squared):\n",
    "\n",
    "  H = np.eye(len(theta))/sigma_p_squared\n",
    "\n",
    "  if X.shape[0] > 0:\n",
    "    logits = np.dot(X, theta)\n",
    "    tmp = np.exp(logits); y_pred = np.divide(tmp, 1+tmp)\n",
    "    D = np.expand_dims(np.multiply(y_pred, 1-y_pred), axis=-1)\n",
    "    H += X.T @ (D * X)\n",
    "\n",
    "  return H\n",
    "\n",
    "\n",
    "\n",
    "class LaplaceAgentLearnFromObs:\n",
    "  \"\"\"Agent based on Laplace approximation.\"\"\"\n",
    "\n",
    "  def __init__(self, num_item: int, num_slot: int, sigma_p_squared: float,\n",
    "               cost_per_slot: float,\n",
    "               ):\n",
    "    self._num_item = num_item  # Total number of items that can be recommended\n",
    "    self._num_slot = num_slot  # Number of available sltos\n",
    "    self._sigma_p_squared = sigma_p_squared\n",
    "    self._num_obs_category = num_item + 1\n",
    "    self._cost_per_slot = cost_per_slot\n",
    "    self.reward_fn = lambda obs, act: float(obs > 0.0) - self._cost_per_slot*np.sum(act)\n",
    "    self.reset()\n",
    "\n",
    "   \n",
    "  def reset(self, seed:int = None):\n",
    "    self._theta_mu = np.zeros(self._num_item)\n",
    "    self._theta_sigma = None  \n",
    "    self._initialization_phase = True\n",
    "    self._action_history = np.array([])\n",
    "    self._observation_history = np.array([])\n",
    "\n",
    "\n",
    "    self._rng = np.random.default_rng(seed)\n",
    "    self._solver = MLR(num_items=self._num_item,\n",
    "                        sigma_p_squared=self._sigma_p_squared)\n",
    " \n",
    "  def select_action(self, verbose=False):\n",
    "    \"\"\"Select an action.\"\"\"\n",
    "     # if in the initialization phase, choose a random action\n",
    "    if self._initialization_phase:\n",
    "      canonical_action = np.concatenate((\n",
    "          np.ones(self._num_slot), np.zeros(self._num_item - self._num_slot)))\n",
    "      action = self._rng.permutation(canonical_action)\n",
    "    else:\n",
    "      theta = self._sample_theta()\n",
    "      # choose action based on sampled theta\n",
    "      action = choose_optimal_assortment(theta, self._num_slot, self._rng,\n",
    "                                         self._cost_per_slot)\n",
    "\n",
    "    if verbose and (t+1)%100==0:\n",
    "      print(f\"Laplace Agent: selected action {action}\")\n",
    "\n",
    "    # check if we have observations with all possible obesrvations\n",
    "    if (self._initialization_phase and\n",
    "        len(np.unique(self._observation_history)) >= self._num_item + 1):\n",
    "       self._initialization_phase = False\n",
    "\n",
    "    return action\n",
    "\n",
    "  def _sample_theta(self):\n",
    "    assert self._theta_mu is not None and self._theta_sigma is not None\n",
    "    theta = self._rng.multivariate_normal(\n",
    "        self._theta_mu,\n",
    "        self._theta_sigma,\n",
    "        tol=1e-5)\n",
    "    return theta\n",
    "\n",
    "  def update(self, action: np.array, observation: int):\n",
    "    if not (self._action_history.size > 0 and self._observation_history.size > 0):\n",
    "      self._action_history = np.expand_dims(action, axis=0)\n",
    "      self._observation_history = np.expand_dims(observation, axis=0)\n",
    "    else:\n",
    "      self._action_history = np.concatenate((self._action_history, action[None, ...]), axis=0)\n",
    "      self._observation_history = np.concatenate((self._observation_history, [observation]), axis=0)\n",
    "    if not self._initialization_phase:\n",
    "\n",
    "      self._theta_mu = self._solver.fit(self._action_history.T, self._observation_history.T)\n",
    "      # compute the inverse Hessian matrix\n",
    "      _, _, hess = self._solver.value_gradient_hessian()\n",
    "      self._theta_sigma = np.linalg.inv(hess + 1e-5 * np.eye(hess.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WT_esDdRzDr8"
   },
   "source": [
    "## Experiment class \n",
    "\n",
    "This helps us to conduct experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjiEI4JuzFo7"
   },
   "outputs": [],
   "source": [
    "def plot_regret(result, timestep_res=50):\n",
    "  cols = ['reward', 'regret', 'cumulative_regret']\n",
    "  df_grp = result.groupby('step')[cols]\n",
    "  df = pd.concat([\n",
    "      df_grp.mean().rename(lambda c: c + '_mean', axis=1),\n",
    "      df_grp.sem().rename(lambda c: c + '_sem', axis=1)],\n",
    "      axis=1).reset_index()\n",
    "  for c in cols:\n",
    "    df[c + '_lbdd'] = df[c + '_mean'] - 2 * df[c + '_sem']\n",
    "    df[c + '_ubdd'] = df[c + '_mean'] + 2 * df[c + '_sem']\n",
    "\n",
    "  p = (\n",
    "      gg.ggplot(df[df.step % timestep_res == 0],\n",
    "                gg.aes(x='step', y='cumulative_regret_mean'))\n",
    "      + gg.geom_line()\n",
    "      + gg.geom_errorbar(gg.aes(ymin='cumulative_regret_lbdd',\n",
    "                                ymax='cumulative_regret_ubdd'))\n",
    "      + gg.xlab('Time')\n",
    "      + gg.ylab('Cumulative regret')\n",
    "  )\n",
    "  return p\n",
    "\n",
    "class Experiment(object):\n",
    "\n",
    "  def __init__(self, agent, environment):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      agent: The agent to train and evaluate.\n",
    "      environment: The environment to train on.\n",
    "    \"\"\"\n",
    "    self._agent = agent\n",
    "    self._env = environment\n",
    "    # self._contextual = contextual \n",
    "    self._result_df = None\n",
    "\n",
    "  def run(self, seeds: Sequence[int], time_horizon: int, verbose: bool = True) -> None:\n",
    "    \"\"\"Runs an agent on an environment.\n",
    "    Args:\n",
    "      time_horizon: Number of steps in each trial.\n",
    "      num_simulation: Number of simulations to run.\n",
    "      verbose: Whether to also log to terminal.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "    for sidx, seed in enumerate(seeds):\n",
    "      print(f'Running simulation {sidx}')\n",
    "      self._env.reset(seed)\n",
    "      self._agent.reset(seed)\n",
    "      cumulative_regret = 0.\n",
    "\n",
    "      action = choose_optimal_assortment(self._env._theta, self._agent._num_slot,\n",
    "                                         self._agent._rng, self._agent._cost_per_slot)\n",
    "      optimal_reward = max(1.0 - self._env.compute_selection_probs(action)[0] \\\n",
    "                           - self._agent._cost_per_slot*np.sum(action), 0)\n",
    "\n",
    "      for tidx in range(time_horizon):\n",
    "        # Generate an action from the agent's policy.\n",
    "      \n",
    "        action = self._agent.select_action()\n",
    "        update_args = ()\n",
    "        # Step the environment.\n",
    "        obs = self._env.step(action)\n",
    "        expected_reward = max(1.0 - self._env.compute_selection_probs(action)[0] \\\n",
    "                              - self._agent._cost_per_slot*np.sum(action), 0)\n",
    "        reward = self._agent.reward_fn(obs, action)\n",
    "        # Update the agent\n",
    "        update_args += (action, obs)\n",
    "        self._agent.update(*update_args)\n",
    "  \n",
    "        # Book keeping\n",
    "        regret = optimal_reward - expected_reward\n",
    "        cumulative_regret += regret\n",
    "        if verbose and tidx % 100 == 0:\n",
    "          regret_stats = {\n",
    "              'instance regret': regret, \n",
    "              'cumulative': cumulative_regret,\n",
    "              'per_step regret': cumulative_regret / (tidx + 1)}\n",
    "          print(f'regret at step {tidx}: {regret_stats}')\n",
    "  \n",
    "        results.append((sidx, tidx, reward, regret, cumulative_regret))\n",
    "    self._result = pd.DataFrame(\n",
    "        results, \n",
    "        columns=['trial', 'step', 'reward', 'regret', 'cumulative_regret'])\n",
    "    return self._result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Isnruv5gaMPk"
   },
   "source": [
    "## Running the experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9AHOZu22yfF"
   },
   "outputs": [],
   "source": [
    "num_item = 10\n",
    "sigma_p_squared = 1.0 \n",
    "num_slot = 5\n",
    "\n",
    "num_simulations = 5\n",
    "horizon = 1001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1xG_kFcYjppy"
   },
   "outputs": [],
   "source": [
    "plt.figure(num_slot)\n",
    "plt.clf()\n",
    "\n",
    "for cost_per_slot in [0.02, 0.1]:\n",
    "  learn_from_reward = False\n",
    "  print('running simulation for number of slots = '+str(num_slot) \n",
    "                        + ' cost per slot = '+str(cost_per_slot))\n",
    "  env = MultiItemRecommendationEnv(num_item, num_slot, sigma_p=np.sqrt(sigma_p_squared))\n",
    "  agent = LaplaceAgentLearnFromObs(num_item, num_slot, sigma_p_squared,\n",
    "                       cost_per_slot=cost_per_slot,\n",
    "                       )\n",
    "  experiment = Experiment(agent, env)\n",
    "  result_obs = experiment.run(range(num_simulations), horizon, verbose=False)\n",
    "  plot_regret(result_obs, timestep_res=50)\n",
    "\n",
    "  res_obs_summary = result_obs.groupby('step').mean()\n",
    "  # colors = ['r', 'g', 'b']\n",
    "\n",
    "  plt.plot(res_obs_summary.index, res_obs_summary.cumulative_regret, # color=colors[1],\\\n",
    "          linestyle='-', linewidth=2, \\\n",
    "          label=f'learning from observation cost per slot: '+str(cost_per_slot))\n",
    "\n",
    "plt.legend(loc=\"best\", fontsize=9)\n",
    "plt.xlabel(\"$t$\", fontsize=14)\n",
    "plt.ylabel(\"cumulative regret\", fontsize=14)\n",
    "plt.title(f'N={num_item}, K={num_slot}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-oXqGLrHG44"
   },
   "source": [
    "# Question 5: Alternatives to maximizing information gain from practice exam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "treEnd7mH8I8"
   },
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dqFyvKu3NYLG"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotnine as gg\n",
    "import random\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import time\n",
    "from typing import Callable, Sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "colors = {0: 'b',\n",
    "          1: 'g',\n",
    "          2: 'r',\n",
    "          3: 'c',\n",
    "          4: 'm',\n",
    "          5: 'y'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rf0FAGdvHQze"
   },
   "outputs": [],
   "source": [
    "class MultipleLogisticEnv(object):\n",
    "  \"\"\"Environment representing a logistic model with multiple concurrent binary outputs.\"\"\"\n",
    "\n",
    "  def __init__(self, action_feature_dim: int, embedding_dim: int, output_dim: int, num_action: int, sigma_p: float = 1.0):\n",
    "    self._action_feature_dim = action_feature_dim\n",
    "    self._embedding_dim = embedding_dim\n",
    "    self._output_dim = output_dim\n",
    "    self._num_action = num_action\n",
    "    self._sigma_p = sigma_p\n",
    "    self._theta = None\n",
    "    self.output_features = None \n",
    "    self.action_features = None \n",
    "\n",
    "  def reset(self, seed: int):\n",
    "    self._rng = np.random.default_rng(seed)\n",
    "    self._theta = self._sigma_p * self._rng.normal(size=(self._action_feature_dim - 1, self._embedding_dim))\n",
    "    self._theta = np.r_[self._theta, -np.ones((1, self._embedding_dim))]\n",
    "    self.output_features = np.abs(self._rng.normal(size=(self._embedding_dim, self._output_dim)))\n",
    "    self.action_features = self._rng.normal(size=(self._action_feature_dim - 1, self._num_action))\n",
    "    self.action_features /= np.linalg.norm(self.action_features, axis=0, keepdims=True)\n",
    "    self.action_features = np.r_[self.action_features, np.ones((1, self._num_action))]\n",
    "\n",
    "  def _validate_action(self, action):\n",
    "    curriculum_idx, practice_idx = action\n",
    "    assert 0 <= curriculum_idx and curriculum_idx < self._num_action\n",
    "    assert 1 <= practice_idx and practice_idx < self._output_dim, 'practice_idx:{}'.format(practice_idx)# practice idx cannot be 0 since that is the official exam.\n",
    "    assert self.action_features is not None, 'Please first reset the environment.'\n",
    "    return curriculum_idx, practice_idx\n",
    "\n",
    "  def step(self, action: np.ndarray):\n",
    "    probs = self.compute_output_probs(action)\n",
    "    return (self._rng.random(len(probs)) <= probs).astype(np.float32)\n",
    "\n",
    "  def compute_output_probs(self, action):\n",
    "    curriculum_idx, practice_idx = self._validate_action(action)\n",
    "    action_feature = self.action_features[:, curriculum_idx]\n",
    "    embedding = action_feature.T @ self._theta\n",
    "    logits = embedding @ self.output_features[:, [0, practice_idx]]\n",
    "    exp_logits = np.exp(logits)\n",
    "    probs = exp_logits / (1 + exp_logits)\n",
    "    return probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vjtnp-zWHQzk"
   },
   "source": [
    "##Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6CSX87aaPzI"
   },
   "source": [
    "### Some utilities for the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4qQAX0NZaNem"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function computes the Hessian matrix of the loss function\n",
    "theta.shape = (dim)\n",
    "X.shape = (num_samples, dim)\n",
    "'''\n",
    "def get_hessian(theta, X, sigma_p_squared):\n",
    "\n",
    "  H = np.eye(len(theta))/sigma_p_squared\n",
    "\n",
    "  if X.shape[0] > 0:\n",
    "    logits = np.dot(X, theta)\n",
    "    tmp = np.exp(logits); y_pred = np.divide(tmp, 1+tmp)\n",
    "    D = np.expand_dims(np.multiply(y_pred, 1-y_pred), axis=-1)\n",
    "    H += X.T @ (D * X)\n",
    "\n",
    "  return H\n",
    "\n",
    "\n",
    "def get_inv_hessian(theta, X, sigma_p_squared):\n",
    "\n",
    "  H = get_hessian(theta, X, sigma_p_squared)\n",
    "  return np.linalg.inv(H)\n",
    "\n",
    "\n",
    "def choose_optimal_action(rng: np.random.Generator, scores: np.ndarray):\n",
    "  assert scores.ndim == 1, 'scores should be 1D array.'\n",
    "  # choose action based on sampled theta\n",
    "  num_action = len(scores)\n",
    "  optimal_actions = np.arange(num_action)[scores == scores.max()]\n",
    "  action = rng.choice(optimal_actions)\n",
    "  return action\n",
    " \n",
    "def get_expected_regret(rng, theta_mu, theta_sigma, action_features_reward):\n",
    "  theta = rng.multivariate_normal(\n",
    "      theta_mu,\n",
    "      theta_sigma,\n",
    "      size=10,\n",
    "      tol=1e-5).T\n",
    "  exp_logits = np.exp(action_features_reward.T @ theta)  # shape=(num_action, num_smaple)\n",
    "  probs = exp_logits / (1.0 + exp_logits)\n",
    "  regrets = probs.max(axis=0, keepdims=True) - probs\n",
    "  return regrets.mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkBj22-TadyO"
   },
   "source": [
    "### Agent based on Laplace Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lhnuDnYeHQzr"
   },
   "outputs": [],
   "source": [
    "class LaplaceAgent:\n",
    "  \"\"\"Agent based on Laplace approximation.\"\"\"\n",
    "\n",
    "  def __init__(self, action_selection_fn: Callable, sigma_p_squared: float):\n",
    "               # learn_from_reward: bool = True):\n",
    "    self._action_selection_fn = action_selection_fn\n",
    "    self._sigma_p_squared = sigma_p_squared\n",
    "\n",
    "  def reset(self, seed, action_features: np.ndarray, output_features: np.ndarray):\n",
    "    self._rng = np.random.default_rng(seed)\n",
    "    self._embedding_dim, self._output_dim = output_features.shape\n",
    "    self._action_features_raw = action_features\n",
    "    self._action_feature_dim_raw, self._num_action = action_features.shape\n",
    "    self._action_feature_dim = self._action_feature_dim_raw * self._embedding_dim\n",
    "    self._output_features = output_features\n",
    "\n",
    "    self._action_features = np.stack([\n",
    "        np.kron(np.eye(self._embedding_dim), self._action_features_raw[:, idx]).T @ self._output_features\n",
    "        for idx in range(self._num_action)], axis=-1)\n",
    "    assert self._action_features.shape == (\n",
    "        self._action_feature_dim,\n",
    "        self._output_dim,\n",
    "        self._num_action)\n",
    "\n",
    "    self._theta_mu = np.zeros(self._action_feature_dim)\n",
    "    self._initialization_phase = True\n",
    "    self._action_feature_history = np.array([])\n",
    "    self._observation_history = np.array([])\n",
    "\n",
    "    self._solver = LogisticRegression(penalty='l2', \n",
    "                                      C=2*self._sigma_p_squared,\n",
    "                                      fit_intercept=False,\n",
    "                                      warm_start=True,\n",
    "                                      max_iter=500)\n",
    " \n",
    "  @property\n",
    "  def action_features(self):\n",
    "    return self._action_features_raw\n",
    "    \n",
    "  @property\n",
    "  def theta_mean_covar(self):\n",
    "    theta_sigma = get_inv_hessian(self._theta_mu, self._action_feature_history, self._sigma_p_squared)\n",
    "    return self._theta_mu, theta_sigma\n",
    "\n",
    "  def select_action(self, verbose=False):\n",
    "    \"\"\"Select an action.\"\"\"\n",
    "     # if in the initialization phase, choose a random action\n",
    "    if self._initialization_phase:\n",
    "      curriculum_idx = self._rng.integers(self._num_action)\n",
    "      practice_idx = self._rng.integers(1, self._output_dim)\n",
    "      action = curriculum_idx, practice_idx\n",
    "    else:\n",
    "      action = self._action_selection_fn(\n",
    "          self._rng,\n",
    "          self._theta_mu,\n",
    "          self._action_features,\n",
    "          self._action_feature_history,\n",
    "          self._sigma_p_squared,\n",
    "          self._output_features)\n",
    "      \n",
    "    if verbose and (t+1)%100==0:\n",
    "      print(f\"Laplace Agent: selected action {action}\")\n",
    "\n",
    "    # check if we have observations with all possible obesrvations\n",
    "    if self._initialization_phase and self._observation_history.size > 0:\n",
    "      if len(np.unique(self._observation_history)) >= 2:\n",
    "        self._initialization_phase = False\n",
    "\n",
    "    return action\n",
    "\n",
    "  def update(self, action: np.array, observation: int):\n",
    "    curriculum_idx, practice_idx = action\n",
    "    action_feature = self._action_features[:, [0, practice_idx], curriculum_idx].T\n",
    "    if not (self._action_feature_history.size > 0 and self._observation_history.size > 0):\n",
    "      self._action_feature_history = action_feature\n",
    "      self._observation_history = np.expand_dims(observation, axis=0)\n",
    "    else:\n",
    "      self._action_feature_history = np.concatenate(\n",
    "          # action_feature already has a batch dim.\n",
    "          (self._action_feature_history, action_feature), axis=0)\n",
    "      self._observation_history = np.concatenate(\n",
    "          (self._observation_history, observation[None, ...]), axis=0)\n",
    "    if not self._initialization_phase:\n",
    "      targets = self._observation_history.flatten()  # flatten is row-major by default.\n",
    "\n",
    "      self._solver.fit(self._action_feature_history, targets)\n",
    "      self._theta_mu = np.squeeze(self._solver.coef_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nq4W_r1GbrRn"
   },
   "outputs": [],
   "source": [
    "def ts_action_selection(\n",
    "    rng: np.random.Generator,\n",
    "    theta_mu: np.ndarray,\n",
    "    action_features: np.ndarray,\n",
    "    action_feature_history: np.ndarray,\n",
    "    sigma_p_squared: float,\n",
    "    output_features: np.ndarray = None):\n",
    "  num_practice = action_features.shape[1] - 1\n",
    "  theta_sigma = get_inv_hessian(theta_mu, action_feature_history, sigma_p_squared)\n",
    "  assert theta_mu is not None\n",
    "  theta = rng.multivariate_normal(\n",
    "      theta_mu,\n",
    "      theta_sigma,\n",
    "      tol=1e-5)\n",
    "  # choose action based on sampled theta\n",
    "  logits = action_features[:, 0, :].T @ theta\n",
    "  curriculum_idx = choose_optimal_action(rng, logits)\n",
    "  practice_idx = rng.integers(1, num_practice)\n",
    "  return curriculum_idx, practice_idx \n",
    "\n",
    "def get_diff_entropy(hessian: np.ndarray):\n",
    "  \"\"\"Computes the expected entropy given theta and negative log posterior hessian.\"\"\"\n",
    "  ld_sign, log_det_hessian = np.linalg.slogdet(hessian)\n",
    "  assert ld_sign >= 0, 'Hessian is not positive semidefinite.'\n",
    "  n = hessian.shape[0]\n",
    "  dentropy = 0.5 * (n * np.log(2 * np.pi * np.e) - log_det_hessian)\n",
    "  return dentropy\n",
    "\n",
    "def get_entropy_reduction(theta_mu, action_features, action_feature_history, sigma_p_squared):\n",
    "    num_action = action_features.shape[-1]\n",
    "    hessian = get_hessian(theta_mu, action_feature_history, sigma_p_squared)\n",
    "    diff_entropy = get_diff_entropy(hessian)\n",
    "    hessian_tp1 = [get_hessian(\n",
    "        theta_mu,\n",
    "        np.r_[action_feature_history, action_features.T[[aidx], :]],\n",
    "        sigma_p_squared)\n",
    "        for aidx in range(num_action)]\n",
    "    diff_entropy_tp1 = np.array([get_diff_entropy(h) for h in hessian_tp1])\n",
    "    entropy_reduction = diff_entropy - diff_entropy_tp1\n",
    "    return entropy_reduction\n",
    "\n",
    "\n",
    "def tswis_action_selection(\n",
    "    rng: np.random.Generator,\n",
    "    theta_mu: np.ndarray,\n",
    "    action_features: np.ndarray,\n",
    "    action_feature_history: np.ndarray,\n",
    "    sigma_p_squared: float,\n",
    "    output_features: np.ndarray):\n",
    "  \"\"\"Thompsong sampling with information seeking for the practice exam.\"\"\"\n",
    "  \n",
    "  # Select the curriculum first based on optimal choice for the exam.\n",
    "  curriculum_idx, _ = ts_action_selection(\n",
    "    rng,\n",
    "    theta_mu,\n",
    "    action_features,\n",
    "    action_feature_history,\n",
    "    sigma_p_squared)\n",
    "    \n",
    "  # Then select practice exam wich maximizes information gain given curriculum_idx\n",
    "  assert theta_mu is not None\n",
    "  action_features_practice = action_features[:, 1:, curriculum_idx]\n",
    "  entropy_reductions = get_entropy_reduction(\n",
    "      theta_mu, action_features_practice, action_feature_history, sigma_p_squared) \n",
    "  practice_idx = choose_optimal_action(rng, entropy_reductions) + 1 \n",
    "  return curriculum_idx, practice_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okHYxdBxyDDx"
   },
   "source": [
    "## PLEASE READ\n",
    "\n",
    "recall from lecture 8 that we introduced a linear form for the observation probability $$\\Pr[O_{t+1, m} = 1| H_t, A_t, \\mathcal{E}] = {\\tt logistic} (\\psi_m^\\top (I \\otimes \\phi_{a_t}) {\\tt vec}(\\theta))$$. In the part of the code that you will be modifying we already did this transformation for you.\n",
    "\n",
    "1. `theta` in this part represents the vectorized for of ${\\tt vec}(\\theta)$. \n",
    "Hence `len(theta)` is equal to number of subjects * curriculum dimension, i.e., $K*d$. `theta_mu` is the estimated mean of this vectorized theta, and `theta_sigma` is the estimated covariance of the vectorized theta.\n",
    "2. `output_features` represents the concatenation of official exam feature and practice exam features $[\\psi_0, \\psi_{1:M}]$. Hence its shape is (number of subjects, number of practice exams + 1), i.e., $(K, M+1)$.\n",
    "3. `action_features` represents the concatenation of the transformed curricula features $\\psi_m^\\top (I \\otimes \\phi_{a}) {\\tt vec}(\\theta)$ along two axes corresponding to $m$ and $a$. Hence, it's shape is (number of subjects * curriculum dimension, number of practice exams +1, number of curricula), i.e., $(K*d, M+1, N)$.\n",
    "\n",
    "Note that in our notation all vectors are assumed column vector.\n",
    "\n",
    "In this form `action_features[:, m, a].T @ theta` represents $\\psi_m^\\top (I \\otimes \\phi_{a}) {\\tt vec}(\\theta)$ which can be used compute $$\\Pr[O_{t+1, m} = 1| H_t, A_t, \\mathcal{E}] = {\\tt logistic} (\\psi_m^\\top (I \\otimes \\phi_{a_t}) {\\tt vec}(\\theta))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9VwSniYGegU"
   },
   "source": [
    "###Thompson sampling with expected practice exam score maximization (**add your code here**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DixsCTvSGdaV"
   },
   "outputs": [],
   "source": [
    "def ts_with_practice_score_maximization(\n",
    "    rng: np.random.Generator,\n",
    "    theta_mu: np.ndarray,\n",
    "    action_features: np.ndarray,\n",
    "    action_feature_history: np.ndarray,\n",
    "    sigma_p_squared: float,\n",
    "    output_features: np.ndarray):\n",
    "  \"\"\"Thompsong sampling withaction_features practice exam score maximization.\"\"\"\n",
    "  \n",
    "  # Select the curriculum first based on optimal choice for the exam.\n",
    "  theta_sigma = get_inv_hessian(theta_mu, action_feature_history, sigma_p_squared)\n",
    "  assert theta_mu is not None\n",
    "  theta = rng.multivariate_normal(\n",
    "      theta_mu,\n",
    "      theta_sigma,\n",
    "      tol=1e-5)\n",
    "  # choose action based on sampled theta\n",
    "  logits = action_features[:, 0, :].T @ theta\n",
    "  curriculum_idx = choose_optimal_action(rng, logits)  \n",
    "  # Then select practice wich maximizes expected practice exam score given curriculum_idx\n",
    "  action_features_practice = action_features[:, 1:, curriculum_idx]\n",
    "  # Note that practice exams features start from index 1 (official exam is index 0).\n",
    " \n",
    "  ############### Start of Code Modification ################################\n",
    "  ### Please implement practice exam selection that chooses exam with highest\n",
    "  ### expected score.\n",
    "  ###########################################################################\n",
    "\n",
    "  practice_idx = ...\n",
    "  ################### End of Code Modification ##############################\n",
    "  \n",
    "  return curriculum_idx, practice_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmmsOpbmIGVE"
   },
   "source": [
    "###Thompson sampling with practice exam similarity maximization (**add your code here**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YOaA_yq7IGVG"
   },
   "outputs": [],
   "source": [
    "def ts_with_practice_similarity_maximization(\n",
    "    rng: np.random.Generator,\n",
    "    theta_mu: np.ndarray,\n",
    "    action_features: np.ndarray,\n",
    "    action_feature_history: np.ndarray,\n",
    "    sigma_p_squared: float,\n",
    "    output_features: np.ndarray):\n",
    "  \"\"\"Thompsong sampling with practice exam similarity maximization.\"\"\"\n",
    "  \n",
    "  # Select the curriculum first based on optimal choice for the exam.\n",
    "  curriculum_idx, _ = ts_action_selection(\n",
    "    rng,\n",
    "    theta_mu,\n",
    "    action_features,\n",
    "    action_feature_history,\n",
    "    sigma_p_squared)\n",
    "    \n",
    "  # Then select practice exam wich maximizes exams' feature similarity given curriculum_idx\n",
    "  assert theta_mu is not None\n",
    "  output_features_practice = output_features[:, 1:] # psi's for practice exam\n",
    "  # Note that practice exams features start from index 1 (official exam is index 0).\n",
    " \n",
    "  ############### Start of Code Modification ################################\n",
    "  ### Please implement practice exam selection that chooses exam with highest\n",
    "  ### similarity to the official exam.\n",
    "  ###########################################################################\n",
    "\n",
    "  practice_idx = ...\n",
    "  ################### End of Code Modification ##############################\n",
    "  \n",
    "  return curriculum_idx, practice_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TswI0z4AIJjk"
   },
   "source": [
    "### Thompson sampling with practice exam logit variance maximization (**add your code here**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ILvrGjixIJjn"
   },
   "outputs": [],
   "source": [
    "def ts_with_practice_score_logit_var_maximization(\n",
    "    rng: np.random.Generator,\n",
    "    theta_mu: np.ndarray,\n",
    "    action_features: np.ndarray,\n",
    "    action_feature_history: np.ndarray,\n",
    "    sigma_p_squared: float, \n",
    "    output_features: np.ndarray):\n",
    "  \"\"\"Thompsong sampling with variance of practice score logit maximization.\"\"\"\n",
    "  \n",
    "  # Select the curriculum first based on optimal choice for the exam.\n",
    "  curriculum_idx, _ = ts_action_selection(\n",
    "    rng,\n",
    "    theta_mu,\n",
    "    action_features,\n",
    "    action_feature_history,\n",
    "    sigma_p_squared)\n",
    "    \n",
    "  # Then select practice wich maximizes the variance of the practice exam's logit given curriculum_idx\n",
    "  assert theta_mu is not None\n",
    "  theta_sigma = get_inv_hessian(theta_mu, action_feature_history, sigma_p_squared)\n",
    "  action_features_practice = action_features[:, 1:, curriculum_idx]\n",
    "  # Note that practice exams features start from index 1 (official exam is index 0).\n",
    " \n",
    "  ############### Start of Code Modification ################################\n",
    "  ### Please implement practice exam selection that chooses exam with highest\n",
    "  ### estimated variance of the logit.\n",
    "  ###########################################################################\n",
    "  \n",
    "  practice_idx = ...\n",
    "  ################### End of Code Modification ##############################\n",
    "  \n",
    "  return curriculum_idx, practice_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xTYPapOHQ0D"
   },
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhNkl6PgHQ0E"
   },
   "outputs": [],
   "source": [
    "class Experiment(object):\n",
    "\n",
    "  def __init__(self, agent, environment):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      agent: The agent to train and evaluate.\n",
    "      environment: The environment to train on.\n",
    "    \"\"\"\n",
    "    self._agent = agent\n",
    "    self._env = environment\n",
    "    self._result_df = None\n",
    "\n",
    "  def run(self, seeds: Sequence[int], time_horizon: int, verbose: bool = True) -> None:\n",
    "    \"\"\"Runs an agent on an environment.\n",
    "    Args:\n",
    "      time_horizon: Number of steps in each trial.\n",
    "      num_simulation: Number of simulations to run.\n",
    "      verbose: Whether to also log to terminal.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "    for sidx, seed in enumerate(seeds):\n",
    "      print(f'Running simulation {sidx}')\n",
    "      start_time = time.time()\n",
    "      self._env.reset(seed)\n",
    "      self._agent.reset(seed, self._env.action_features, self._env.output_features)\n",
    "      cumulative_regret = 0.\n",
    "      num_action = self._agent.action_features.shape[-1]\n",
    "      expected_rewards = np.array([\n",
    "          self._env.compute_output_probs((aidx, 1))[0] for aidx in range(num_action)])\n",
    "      optimal_reward = expected_rewards.max()\n",
    "\n",
    "      for tidx in range(time_horizon):\n",
    "        # Generate an action from the agent's policy.\n",
    "        action = self._agent.select_action()\n",
    "        # Step the environment.\n",
    "        obs = self._env.step(action)\n",
    "        expected_reward = self._env.compute_output_probs(action)[0]\n",
    "        reward = obs[0]\n",
    "        # Update the agent\n",
    "        self._agent.update(action, obs)\n",
    "  \n",
    "        # Book keeping\n",
    "        regret = optimal_reward - expected_reward\n",
    "        cumulative_regret += regret\n",
    "        if verbose and tidx % 100 == 0:\n",
    "          regret_stats = {\n",
    "              'instance': regret, \n",
    "              'cumulative': cumulative_regret,\n",
    "              'per_step': cumulative_regret / (tidx + 1)}\n",
    "          print(f'regret at step {tidx}: {regret_stats}')\n",
    "  \n",
    "        results.append((sidx, tidx, reward, regret, cumulative_regret))\n",
    "      print(f'experiment {sidx} took {time.time() - start_time} seconds.')\n",
    "\n",
    "    self._result = pd.DataFrame(\n",
    "        results, \n",
    "        columns=['trial', 'step', 'reward', 'regret', 'cumulative_regret'])\n",
    "    return self._result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jtn25mG4HQ0G"
   },
   "outputs": [],
   "source": [
    "action_feature_dim = 20  # Dimension of curricula\n",
    "embedding_dim = 2  # Number of subjects\n",
    "output_dim = 50  # Number of exams\n",
    "num_action = 50\n",
    "sigma_p_squared = 1\n",
    "\n",
    "num_experiment = 5\n",
    "horizon = 1001\n",
    "seeds = range(num_experiment)\n",
    "\n",
    "env = MultipleLogisticEnv(\n",
    "    action_feature_dim=action_feature_dim,\n",
    "    embedding_dim=embedding_dim,\n",
    "    output_dim=output_dim,\n",
    "    num_action=num_action,\n",
    "    sigma_p=np.sqrt(sigma_p_squared))\n",
    " \n",
    "def experiment_factory(action_selection_fn, env=env):\n",
    "  agent = LaplaceAgent(\n",
    "      action_selection_fn,\n",
    "      sigma_p_squared=sigma_p_squared)\n",
    "  return Experiment(agent, env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-Iu99PhYdMb"
   },
   "outputs": [],
   "source": [
    "action_selection_fn = tswis_action_selection\n",
    "experiment = experiment_factory(action_selection_fn)\n",
    "result_tswis = experiment.run(seeds, horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ZFdDhIhL8Vs"
   },
   "outputs": [],
   "source": [
    "action_selection_fn = ts_action_selection\n",
    "experiment = experiment_factory(action_selection_fn)\n",
    "result_ts = experiment.run(seeds, horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhCxPL8Yg1A8"
   },
   "outputs": [],
   "source": [
    "res_ts_summary = result_ts.groupby('step').mean()\n",
    "res_tswis_summary = result_tswis.groupby('step').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEgdrGM9N1ce"
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.clf()\n",
    "\n",
    "plt.plot(res_ts_summary.index, res_ts_summary.cumulative_regret, color=colors[0],\\\n",
    "         linestyle='-', linewidth=2, label=f'TS')\n",
    "plt.plot(res_tswis_summary.index, res_tswis_summary.cumulative_regret, color=colors[1],\\\n",
    "         linestyle='-', linewidth=2, label=f'TS with info seeking')\n",
    "\n",
    "plt.legend(loc=\"best\", fontsize=9)\n",
    "plt.xlabel(\"$t$\", fontsize=14)\n",
    "plt.ylabel(\"cumulative regret\", fontsize=14)\n",
    "plt.title(f'N={action_feature_dim}, K={embedding_dim}, M={output_dim}')\n",
    "\n",
    "# save the plot\n",
    "filename_image = f'ai_tutor_cumulative_regret.pdf'\n",
    "plt.savefig(filename_image)\n",
    "files.download(filename_image)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MgI1_N6FIDNY",
    "Ho_DdegKoQRl",
    "ruSOPgrHy7Do",
    "wruhIqzbqD1_",
    "_-oXqGLrHG44"
   ],
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
